[
  {
    "body": "## Summary\n\nImplement an LLM judge alignment system to ensure automated evaluation graders produce results consistent with human judgment. This extends issue #238 (Evals Framework) with quality assurance for the grading process itself.\n\n## Background\n\nFrom OpenAI's \"Building Resilient Prompts\" cookbook:\n\n> \"After analyzing labeled samples, you can work to automate the process of evaluation by using LLMs as graders. These LLMs (referred to as \"LLM judges\") can quickly score outputs, check for specific criteria, and apply human-like judgment to a wide range of evaluation tasks.\"\n\nThe key insight: **An LLM judge is only useful if it agrees with human evaluators.** We need to measure and optimize this alignment.\n\n---\n\n## Alignment Metrics\n\n### Core Metrics\n\n| Metric | Formula | Target | Description |\n|--------|---------|--------|-------------|\n| **TPR** | True Positives / All Positives | > 80% | Does judge catch correct outputs? |\n| **TNR** | True Negatives / All Negatives | > 80% | Does judge catch incorrect outputs? |\n| **Accuracy** | (TP + TN) / Total | > 85% | Overall agreement with humans |\n\n### Why Both TPR and TNR?\n\nA judge that always says \"PASS\" would have 100% TPR but 0% TNR. We need both to be useful:\n\n```\n                    Human Label\n                    PASS    FAIL\nJudge Says PASS     TP      FP     ← Low TNR: misses bad outputs\nJudge Says FAIL     FN      TN     ← Low TPR: misses good outputs\n```\n\n---\n\n## Data Split Strategy\n\n```\nHuman-labeled examples (100+ total)\n              ↓\n┌──────────────────────────────────────────────┐\n│   TRAIN    │   VALIDATION   │     TEST       │\n│   (20%)    │     (40%)      │    (40%)       │\n│            │                │                │\n│ Few-shot   │ Tune prompt    │ Final metrics  │\n│ examples   │ until pass     │ (never tune)   │\n└──────────────────────────────────────────────┘\n```\n\n### Why This Split?\n\n- **Train (20%)**: Small set for few-shot examples in judge prompt\n- **Validation (40%)**: Iterate on prompt until TPR/TNR targets met\n- **Test (40%)**: Final evaluation - **never tune on this set**\n\nThe test set acts as holdout to detect overfitting to validation data.\n\n---\n\n## Implementation\n\n### 1. Judge Configuration\n\n```typescript\n// src/core/evals/judge-config.ts\nexport interface JudgeConfig {\n  model: string;                    // e.g., \"gpt-5.2\"\n  targetTPR: number;                // e.g., 0.8\n  targetTNR: number;                // e.g., 0.8\n  trainExamples: LabeledExample[];  // 20% of data\n  validationSet: LabeledExample[];  // 40% of data\n  testSet: LabeledExample[];        // 40% of data\n}\n\nexport interface LabeledExample {\n  input: string;          // The input to the task\n  output: string;         // The output being graded\n  humanLabel: \"pass\" | \"fail\";\n  reason?: string;        // Why human labeled this way\n}\n```\n\n### 2. Judge Alignment Process\n\n```typescript\n// src/core/evals/judge-alignment.ts\nexport class JudgeAligner {\n  private config: JudgeConfig;\n  private currentPrompt: string;\n  \n  constructor(config: JudgeConfig) {\n    this.config = config;\n    this.currentPrompt = this.buildInitialPrompt();\n  }\n\n  async align(): Promise<AlignedJudge> {\n    let iteration = 0;\n    let metrics = await this.evaluateOnValidation();\n    \n    console.log(`[Judge] Initial: TPR=${metrics.tpr}, TNR=${metrics.tnr}`);\n    \n    while (\n      (metrics.tpr < this.config.targetTPR || \n       metrics.tnr < this.config.targetTNR) && \n      iteration < 10\n    ) {\n      // Analyze failures\n      const failures = await this.analyzeFailures(metrics);\n      \n      // Adjust prompt based on failure patterns\n      this.currentPrompt = await this.improvePrompt(failures);\n      \n      // Re-evaluate\n      metrics = await this.evaluateOnValidation();\n      iteration++;\n      \n      console.log(`[Judge] Iteration ${iteration}: TPR=${metrics.tpr}, TNR=${metrics.tnr}`);\n    }\n    \n    // Final evaluation on test set (never tuned on this)\n    const finalMetrics = await this.evaluateOnTest();\n    console.log(`[Judge] Final (test): TPR=${finalMetrics.tpr}, TNR=${finalMetrics.tnr}`);\n    \n    return new AlignedJudge({\n      prompt: this.currentPrompt,\n      model: this.config.model,\n      metrics: finalMetrics,\n    });\n  }\n\n  private buildInitialPrompt(): string {\n    const fewShotExamples = this.config.trainExamples\n      .slice(0, 5)  // Use top 5 from train set\n      .map(ex => `\nInput: ${ex.input}\nOutput: ${ex.output}\nGrade: ${ex.humanLabel.toUpperCase()}\nReason: ${ex.reason ?? \"N/A\"}\n`).join(\"\\n---\\n\");\n\n    return `You are a code quality judge. Grade the following output as PASS or FAIL.\n\n## Examples\n\n${fewShotExamples}\n\n## Instructions\n\n- PASS: Output correctly solves the task\n- FAIL: Output has errors, is incomplete, or doesn't meet requirements\n\nRespond with:\n- Grade: PASS or FAIL\n- Reason: Brief explanation\n`;\n  }\n\n  private async evaluateOnValidation(): Promise<JudgeMetrics> {\n    return this.evaluate(this.config.validationSet);\n  }\n\n  private async evaluateOnTest(): Promise<JudgeMetrics> {\n    return this.evaluate(this.config.testSet);\n  }\n\n  private async evaluate(examples: LabeledExample[]): Promise<JudgeMetrics> {\n    let tp = 0, fp = 0, fn = 0, tn = 0;\n    \n    for (const example of examples) {\n      const judgeResult = await this.runJudge(example);\n      const humanLabel = example.humanLabel;\n      \n      if (judgeResult === \"pass\" && humanLabel === \"pass\") tp++;\n      else if (judgeResult === \"pass\" && humanLabel === \"fail\") fp++;\n      else if (judgeResult === \"fail\" && humanLabel === \"pass\") fn++;\n      else if (judgeResult === \"fail\" && humanLabel === \"fail\") tn++;\n    }\n    \n    const tpr = tp / (tp + fn) || 0;  // Sensitivity\n    const tnr = tn / (tn + fp) || 0;  // Specificity\n    const accuracy = (tp + tn) / examples.length;\n    \n    return { tp, fp, fn, tn, tpr, tnr, accuracy };\n  }\n\n  private async runJudge(example: LabeledExample): Promise<\"pass\" | \"fail\"> {\n    const response = await llm.complete({\n      model: this.config.model,\n      messages: [\n        { role: \"system\", content: this.currentPrompt },\n        { role: \"user\", content: `Input: ${example.input}\\nOutput: ${example.output}` },\n      ],\n    });\n    \n    const grade = response.toLowerCase().includes(\"grade: pass\") ? \"pass\" : \"fail\";\n    return grade;\n  }\n\n  private async analyzeFailures(metrics: JudgeMetrics): Promise<FailureAnalysis> {\n    // Identify false positives and false negatives\n    // Group by failure pattern using Open Coding → Axial Coding\n    // Return patterns for prompt improvement\n  }\n\n  private async improvePrompt(failures: FailureAnalysis): Promise<string> {\n    // Add examples of failure cases\n    // Clarify ambiguous criteria\n    // Add explicit rules for edge cases\n  }\n}\n```\n\n### 3. Grader Types\n\n```typescript\n// src/core/evals/graders.ts\nexport type GraderType = \n  | \"string_check\"    // Exact or contains match\n  | \"text_similarity\" // Cosine similarity threshold\n  | \"score_model\"     // Numeric rating (1-5)\n  | \"label_model\"     // Classification (pass/fail)\n  | \"python_code\";    // Custom Python logic\n\nexport interface Grader {\n  type: GraderType;\n  evaluate(input: string, output: string): Promise<GradeResult>;\n}\n\n// String check grader\nexport class StringCheckGrader implements Grader {\n  type: GraderType = \"string_check\";\n  \n  constructor(private criteria: {\n    contains?: string[];\n    notContains?: string[];\n    exact?: string;\n  }) {}\n  \n  async evaluate(input: string, output: string): Promise<GradeResult> {\n    let pass = true;\n    const reasons: string[] = [];\n    \n    if (this.criteria.exact !== undefined) {\n      pass = output === this.criteria.exact;\n      if (!pass) reasons.push(\"Does not match exact expected value\");\n    }\n    \n    if (this.criteria.contains) {\n      for (const term of this.criteria.contains) {\n        if (!output.includes(term)) {\n          pass = false;\n          reasons.push(`Missing required term: ${term}`);\n        }\n      }\n    }\n    \n    if (this.criteria.notContains) {\n      for (const term of this.criteria.notContains) {\n        if (output.includes(term)) {\n          pass = false;\n          reasons.push(`Contains forbidden term: ${term}`);\n        }\n      }\n    }\n    \n    return { \n      grade: pass ? \"pass\" : \"fail\", \n      reason: reasons.join(\"; \") || \"All criteria met\" \n    };\n  }\n}\n\n// LLM label grader (uses aligned judge)\nexport class LabelModelGrader implements Grader {\n  type: GraderType = \"label_model\";\n  \n  constructor(private judge: AlignedJudge) {}\n  \n  async evaluate(input: string, output: string): Promise<GradeResult> {\n    return this.judge.grade(input, output);\n  }\n}\n```\n\n### 4. Failure Mode Analysis (Open/Axial Coding)\n\n```typescript\n// src/core/evals/failure-analysis.ts\nexport interface FailureCase {\n  example: LabeledExample;\n  judgeResult: \"pass\" | \"fail\";\n  humanLabel: \"pass\" | \"fail\";\n  errorType: \"false_positive\" | \"false_negative\";\n}\n\nexport class FailureAnalyzer {\n  // Step 1: Open Coding - tag each failure with descriptive labels\n  async openCode(failures: FailureCase[]): Promise<TaggedFailure[]> {\n    const tagged: TaggedFailure[] = [];\n    \n    for (const failure of failures) {\n      // Use LLM to identify why the judge was wrong\n      const analysis = await llm.complete({\n        model: \"gpt-5.2\",\n        messages: [\n          { role: \"system\", content: `Analyze why this grading was incorrect.` },\n          { role: \"user\", content: `\nInput: ${failure.example.input}\nOutput: ${failure.example.output}\nJudge said: ${failure.judgeResult}\nHuman said: ${failure.humanLabel}\n\nWhat specific aspect did the judge miss or misunderstand?\nProvide a short tag (2-4 words) describing the failure mode.\n` }\n        ]\n      });\n      \n      tagged.push({\n        ...failure,\n        tag: analysis.trim(),\n      });\n    }\n    \n    return tagged;\n  }\n  \n  // Step 2: Axial Coding - group tags into categories\n  async axialCode(tagged: TaggedFailure[]): Promise<FailureCategory[]> {\n    const tagCounts = new Map<string, number>();\n    \n    for (const failure of tagged) {\n      tagCounts.set(failure.tag, (tagCounts.get(failure.tag) ?? 0) + 1);\n    }\n    \n    // Group similar tags using LLM\n    const categories = await llm.complete({\n      model: \"gpt-5.2\",\n      messages: [\n        { role: \"system\", content: \"Group these failure tags into 3-5 categories.\" },\n        { role: \"user\", content: `Tags and counts:\\n${\n          Array.from(tagCounts.entries())\n            .map(([tag, count]) => `- ${tag}: ${count}`)\n            .join(\"\\n\")\n        }` }\n      ]\n    });\n    \n    return this.parseCategories(categories);\n  }\n}\n```\n\n---\n\n## AutoDev Grader Examples\n\n| Task Output | Grader | Criteria |\n|-------------|--------|----------|\n| **Diff validity** | Python code | `parse_diff(output) != null` |\n| **Test result** | String check | Contains \"PASS\" or \"FAIL\" |\n| **Code quality** | Score model | Rating 1-5, pass if >= 3 |\n| **DoD completion** | Label model | All acceptance criteria met |\n| **PR description** | Label model | Clear, complete, follows template |\n\n### Example: Diff Quality Grader\n\n```typescript\nconst diffQualityGrader = new LabelModelGrader(\n  await alignJudge({\n    model: \"gpt-5.2\",\n    targetTPR: 0.85,\n    targetTNR: 0.85,\n    trainExamples: [\n      { input: \"Add logging\", output: validDiff, humanLabel: \"pass\" },\n      { input: \"Add logging\", output: syntaxErrorDiff, humanLabel: \"fail\" },\n      // ... more examples\n    ],\n    validationSet: [...],  // 40% of labeled examples\n    testSet: [...],        // 40% of labeled examples\n  })\n);\n```\n\n---\n\n## Configuration\n\n```bash\n# Enable evals with judge alignment\nENABLE_EVALS=true\nEVAL_JUDGE_MODEL=gpt-5.2\nEVAL_TARGET_TPR=0.8\nEVAL_TARGET_TNR=0.8\nEVAL_MAX_ALIGNMENT_ITERATIONS=10\n```\n\n---\n\n## Database Schema\n\n```sql\n-- Store aligned judge configurations\nCREATE TABLE eval_judges (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  name VARCHAR(100) NOT NULL,\n  model VARCHAR(100) NOT NULL,\n  prompt TEXT NOT NULL,\n  \n  -- Alignment metrics\n  tpr DECIMAL(5,4),\n  tnr DECIMAL(5,4),\n  accuracy DECIMAL(5,4),\n  \n  -- Data split sizes\n  train_count INT,\n  validation_count INT,\n  test_count INT,\n  \n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Store grading results\nCREATE TABLE eval_grades (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  judge_id UUID REFERENCES eval_judges(id),\n  task_id UUID REFERENCES tasks(id),\n  \n  input TEXT NOT NULL,\n  output TEXT NOT NULL,\n  grade VARCHAR(10) NOT NULL,  -- pass/fail\n  reason TEXT,\n  \n  -- For alignment feedback\n  human_override VARCHAR(10),  -- null if human agreed\n  \n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n---\n\n## Acceptance Criteria\n\n- [ ] JudgeConfig and LabeledExample types\n- [ ] JudgeAligner class with TPR/TNR optimization\n- [ ] String check grader\n- [ ] Label model grader (using aligned judge)\n- [ ] Failure analysis with Open/Axial coding\n- [ ] Database schema for judges and grades\n- [ ] API endpoint for running evals\n- [ ] Dashboard integration for reviewing grades\n- [ ] Documentation for creating custom graders\n\n## Complexity\n\n**M** - New subsystem, requires labeled data, iterative alignment\n\n## Dependencies\n\n- Issue #238 (Evals Framework) - provides base infrastructure\n\n## References\n\n- [OpenAI Building Resilient Prompts](https://cookbook.openai.com/articles/techniques_to_improve_reliability)\n- [OpenAI Datasets & Evals](https://platform.openai.com/docs/guides/evals)\n",
    "number": 246,
    "title": "feat: Implement LLM Judge Alignment for Evals Quality",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/246"
  },
  {
    "body": "## Summary\n\nIntegrate OpenAI's Computer Use Agent (CUA) to enable visual testing of UI changes, automated browser testing, and screenshot-based validation.\n\n## Background\n\nFrom OpenAI's Computer Use documentation:\n\n> \"Computer use is a practical application of our Computer-Using Agent (CUA) model, which combines vision capabilities with advanced reasoning to simulate controlling computer interfaces and performing tasks.\"\n\nAutoDev could use CUA to:\n1. Visually verify UI changes after applying diffs\n2. Run E2E tests by interacting with the actual application\n3. Capture screenshots for documentation\n4. Validate that changes don't break the UI\n\n---\n\n## CUA Loop Architecture\n\n```\nUser Goal: \"Verify the new button appears on the homepage\"\n                    ↓\n┌─────────────────────────────────────────────────────────┐\n│                     CUA LOOP                             │\n├─────────────────────────────────────────────────────────┤\n│  1. Send screenshot + goal to computer-use-preview      │\n│                    ↓                                    │\n│  2. Model returns action (click, type, scroll, wait)    │\n│                    ↓                                    │\n│  3. Execute action in browser (Playwright)              │\n│                    ↓                                    │\n│  4. Capture new screenshot                              │\n│                    ↓                                    │\n│  5. Send screenshot back to model                       │\n│                    ↓                                    │\n│  6. Repeat until goal achieved or model stops           │\n└─────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Core Implementation\n\n### 1. ComputerUseAgent Class\n\n```typescript\n// src/agents/computer-use.ts\nimport OpenAI from \"openai\";\nimport { chromium, Page, Browser } from \"playwright\";\n\ninterface CUAAction {\n  type: \"click\" | \"double_click\" | \"scroll\" | \"type\" | \"keypress\" | \"wait\" | \"screenshot\" | \"drag\";\n  x?: number;\n  y?: number;\n  button?: \"left\" | \"right\" | \"wheel\" | \"back\" | \"forward\";\n  text?: string;\n  keys?: string[];\n  scrollX?: number;\n  scrollY?: number;\n  startX?: number;\n  startY?: number;\n  path?: Array<{ x: number; y: number }>;\n}\n\ninterface CUAResult {\n  success: boolean;\n  actions: CUAAction[];\n  screenshots: string[];\n  finalOutput: any;\n  error?: string;\n}\n\nexport class ComputerUseAgent {\n  private client: OpenAI;\n  private browser: Browser | null = null;\n  private page: Page | null = null;\n  private maxActions: number;\n\n  constructor(options?: { maxActions?: number }) {\n    this.client = new OpenAI({\n      timeout: 15 * 60 * 1000,  // 15 min timeout for CUA\n    });\n    this.maxActions = options?.maxActions ?? 50;\n  }\n\n  async startBrowser(url: string): Promise<void> {\n    this.browser = await chromium.launch({\n      headless: process.env.CUA_BROWSER_HEADLESS !== \"false\",\n      chromiumSandbox: true,\n    });\n    this.page = await this.browser.newPage();\n    await this.page.setViewportSize({ width: 1024, height: 768 });\n    await this.page.goto(url);\n  }\n\n  async executeGoal(goal: string): Promise<CUAResult> {\n    const screenshot = await this.captureScreenshot();\n\n    const response = await this.client.responses.create({\n      model: \"computer-use-preview\",\n      tools: [{\n        type: \"computer_use_preview\",\n        display_width: 1024,\n        display_height: 768,\n        environment: \"browser\",\n      }],\n      input: [{\n        role: \"user\",\n        content: [\n          { type: \"input_text\", text: goal },\n          { \n            type: \"input_image\", \n            image_url: `data:image/png;base64,${screenshot}` \n          },\n        ],\n      }],\n      reasoning: { summary: \"concise\" },\n      truncation: \"auto\",\n    });\n\n    return this.runCUALoop(response);\n  }\n\n  private async runCUALoop(response: any): Promise<CUAResult> {\n    const actions: CUAAction[] = [];\n    const screenshots: string[] = [];\n    let actionCount = 0;\n\n    while (actionCount < this.maxActions) {\n      const computerCalls = response.output.filter(\n        (item: any) => item.type === \"computer_call\"\n      );\n\n      if (computerCalls.length === 0) {\n        // Goal achieved or model stopped\n        break;\n      }\n\n      const call = computerCalls[0];\n      const action = call.action as CUAAction;\n\n      // Handle safety checks\n      const safetyResult = await this.handleSafetyChecks(call);\n      if (!safetyResult.proceed) {\n        return {\n          success: false,\n          actions,\n          screenshots,\n          finalOutput: response.output,\n          error: `Safety check blocked: ${safetyResult.reason}`,\n        };\n      }\n\n      // Execute action\n      await this.executeAction(action);\n      actions.push(action);\n      actionCount++;\n\n      // Capture new screenshot\n      await this.page!.waitForTimeout(1000);\n      const newScreenshot = await this.captureScreenshot();\n      screenshots.push(newScreenshot);\n\n      // Send back to model\n      response = await this.client.responses.create({\n        model: \"computer-use-preview\",\n        previous_response_id: response.id,\n        tools: [{\n          type: \"computer_use_preview\",\n          display_width: 1024,\n          display_height: 768,\n          environment: \"browser\",\n        }],\n        input: [{\n          call_id: call.call_id,\n          type: \"computer_call_output\",\n          output: {\n            type: \"input_image\",\n            image_url: `data:image/png;base64,${newScreenshot}`,\n          },\n        }],\n        truncation: \"auto\",\n        ...(safetyResult.acknowledged && {\n          acknowledged_safety_checks: safetyResult.acknowledged,\n        }),\n      });\n    }\n\n    return { \n      success: true, \n      actions, \n      screenshots, \n      finalOutput: response.output \n    };\n  }\n\n  private async handleSafetyChecks(call: any): Promise<{\n    proceed: boolean;\n    reason?: string;\n    acknowledged?: Array<{ id: string; code: string; message: string }>;\n  }> {\n    const pendingChecks = call.pending_safety_checks ?? [];\n    \n    if (pendingChecks.length === 0) {\n      return { proceed: true };\n    }\n\n    // Safety check codes:\n    // - malicious_instructions: User trying to do something harmful\n    // - irrelevant_domain: Navigating away from allowed URLs\n    // - sensitive_domain: Banking, government, healthcare sites\n    \n    const acknowledged = [];\n    for (const check of pendingChecks) {\n      console.log(`[CUA] Safety check: ${check.code} - ${check.message}`);\n      \n      // Only acknowledge irrelevant_domain if URL is in allowlist\n      if (check.code === \"irrelevant_domain\") {\n        const allowedUrls = (process.env.CUA_ALLOWED_URLS ?? \"localhost\").split(\",\");\n        const currentUrl = await this.page!.url();\n        const isAllowed = allowedUrls.some(url => currentUrl.includes(url));\n        \n        if (isAllowed) {\n          acknowledged.push({\n            id: check.id,\n            code: check.code,\n            message: check.message,\n          });\n        } else {\n          return { \n            proceed: false, \n            reason: `URL not in allowlist: ${currentUrl}` \n          };\n        }\n      }\n      \n      // Never acknowledge malicious_instructions or sensitive_domain\n      if (check.code === \"malicious_instructions\" || check.code === \"sensitive_domain\") {\n        return { \n          proceed: false, \n          reason: `Blocked by ${check.code}: ${check.message}` \n        };\n      }\n    }\n\n    return { proceed: true, acknowledged };\n  }\n\n  private async executeAction(action: CUAAction): Promise<void> {\n    if (!this.page) throw new Error(\"Browser not started\");\n\n    switch (action.type) {\n      case \"click\":\n        await this.page.mouse.click(action.x!, action.y!, { \n          button: action.button ?? \"left\" \n        });\n        break;\n        \n      case \"double_click\":\n        await this.page.mouse.dblclick(action.x!, action.y!, { \n          button: action.button ?? \"left\" \n        });\n        break;\n        \n      case \"type\":\n        await this.page.keyboard.type(action.text!);\n        break;\n        \n      case \"scroll\":\n        await this.page.mouse.move(action.x!, action.y!);\n        await this.page.evaluate(\n          ([scrollX, scrollY]) => window.scrollBy(scrollX, scrollY),\n          [action.scrollX ?? 0, action.scrollY ?? 0]\n        );\n        break;\n        \n      case \"keypress\":\n        for (const key of action.keys ?? []) {\n          await this.page.keyboard.press(key);\n        }\n        break;\n        \n      case \"wait\":\n        await this.page.waitForTimeout(2000);\n        break;\n        \n      case \"drag\":\n        if (action.path && action.path.length > 0) {\n          await this.page.mouse.move(action.startX!, action.startY!);\n          await this.page.mouse.down();\n          for (const point of action.path) {\n            await this.page.mouse.move(point.x, point.y);\n          }\n          await this.page.mouse.up();\n        }\n        break;\n        \n      case \"screenshot\":\n        // Just capture, don't do anything\n        break;\n    }\n  }\n\n  private async captureScreenshot(): Promise<string> {\n    if (!this.page) throw new Error(\"Browser not started\");\n    const buffer = await this.page.screenshot();\n    return buffer.toString(\"base64\");\n  }\n\n  async close(): Promise<void> {\n    await this.browser?.close();\n  }\n}\n```\n\n---\n\n## Docker Setup for Sandboxed Execution\n\nFor production, run CUA in an isolated Docker container:\n\n### Dockerfile.cua\n\n```dockerfile\nFROM mcr.microsoft.com/playwright:v1.40.0-jammy\n\nWORKDIR /app\n\n# Install dependencies\nCOPY package.json bun.lockb ./\nRUN npm install\n\n# Copy source\nCOPY src/ ./src/\n\n# Security: Run as non-root user\nRUN useradd -m cuauser\nUSER cuauser\n\n# Environment\nENV CUA_BROWSER_HEADLESS=true\nENV CUA_ALLOWED_URLS=localhost\n\nCMD [\"bun\", \"run\", \"src/services/cua-worker.ts\"]\n```\n\n### Docker Compose\n\n```yaml\nservices:\n  cua-worker:\n    build:\n      context: .\n      dockerfile: Dockerfile.cua\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - CUA_ALLOWED_URLS=localhost,staging.example.com\n    security_opt:\n      - seccomp:unconfined  # Required for Chromium\n    cap_drop:\n      - ALL\n    cap_add:\n      - SYS_ADMIN  # Required for Chromium sandbox\n    networks:\n      - internal\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n          cpus: \"1.0\"\n```\n\n---\n\n## Integration with Foreman\n\n```typescript\n// src/services/foreman.ts\nexport class Foreman {\n  private cuaAgent?: ComputerUseAgent;\n\n  async runVisualTests(\n    appUrl: string,\n    testCases: VisualTestCase[]\n  ): Promise<VisualTestResults> {\n    // Validate URL is allowed\n    const allowedUrls = (process.env.CUA_ALLOWED_URLS ?? \"\").split(\",\");\n    if (!allowedUrls.some(url => appUrl.includes(url))) {\n      throw new Error(`URL not allowed for CUA: ${appUrl}`);\n    }\n\n    this.cuaAgent = new ComputerUseAgent({ maxActions: 50 });\n    await this.cuaAgent.startBrowser(appUrl);\n\n    const results: VisualTestResult[] = [];\n    for (const testCase of testCases) {\n      try {\n        const result = await this.cuaAgent.executeGoal(testCase.goal);\n        results.push({\n          testCase,\n          passed: this.evaluateResult(result, testCase.expectedOutcome),\n          screenshots: result.screenshots,\n          actions: result.actions,\n          error: result.error,\n        });\n      } catch (error) {\n        results.push({\n          testCase,\n          passed: false,\n          screenshots: [],\n          actions: [],\n          error: error.message,\n        });\n      }\n    }\n\n    await this.cuaAgent.close();\n    return { \n      results, \n      passRate: this.calculatePassRate(results) \n    };\n  }\n\n  private evaluateResult(\n    result: CUAResult, \n    expected: string\n  ): boolean {\n    // Check if final output indicates success\n    const textOutputs = result.finalOutput.filter(\n      (o: any) => o.type === \"text\"\n    );\n    const combinedText = textOutputs.map((o: any) => o.text).join(\" \");\n    \n    // Simple keyword matching for now\n    // Could use LLM to evaluate semantic similarity\n    return combinedText.toLowerCase().includes(\"success\") ||\n           combinedText.toLowerCase().includes(\"verified\") ||\n           combinedText.toLowerCase().includes(\"found\");\n  }\n\n  private calculatePassRate(results: VisualTestResult[]): number {\n    const passed = results.filter(r => r.passed).length;\n    return results.length > 0 ? (passed / results.length) * 100 : 0;\n  }\n}\n```\n\n---\n\n## Use Cases for AutoDev\n\n### 1. Visual Regression Testing\n\n```typescript\n// After applying diff, verify UI hasn't broken\nconst cua = new ComputerUseAgent();\nawait cua.startBrowser(\"http://localhost:3000\");\nconst result = await cua.executeGoal(\n  \"Navigate to the homepage and verify the main navigation is visible and functional\"\n);\n```\n\n### 2. E2E Test Execution\n\n```typescript\n// Run acceptance criteria as visual tests\nconst result = await cua.executeGoal(`\n  Test the user login flow:\n  1. Click the \"Login\" button\n  2. Enter email \"test@example.com\"\n  3. Enter password \"password123\"\n  4. Click \"Submit\"\n  5. Verify you see the dashboard\n`);\n```\n\n### 3. Screenshot Documentation\n\n```typescript\n// Capture screenshots of new features\nconst result = await cua.executeGoal(\n  \"Navigate to the new settings page and take a screenshot showing the dark mode toggle\"\n);\n```\n\n---\n\n## Safety Considerations\n\n### Security Checklist\n\n1. **Sandboxed browser**: Always use Chromium sandbox mode\n2. **No credentials**: Never expose real auth tokens to CUA\n3. **Allowlist URLs**: Only test known/safe URLs via `CUA_ALLOWED_URLS`\n4. **Handle safety checks**: Never acknowledge `malicious_instructions` or `sensitive_domain`\n5. **Timeout limits**: Cap CUA loop iterations (`maxActions: 50`)\n6. **Docker isolation**: Run in isolated container with limited resources\n7. **Non-root user**: Execute as non-privileged user in Docker\n\n### Safety Check Types\n\n| Code | Description | Action |\n|------|-------------|--------|\n| `malicious_instructions` | User trying harmful actions | **Block always** |\n| `irrelevant_domain` | Navigating away from task | Check allowlist |\n| `sensitive_domain` | Banking, gov, healthcare | **Block always** |\n\n---\n\n## Configuration\n\n```bash\n# Environment variables\nENABLE_COMPUTER_USE=true\nOPENAI_API_KEY=sk-...\nCUA_TIMEOUT_MS=900000              # 15 minutes\nCUA_MAX_ACTIONS=50                 # Max actions per goal\nCUA_BROWSER_HEADLESS=true\nCUA_ALLOWED_URLS=localhost,staging.example.com\n```\n\n---\n\n## Database Schema\n\n```sql\nCREATE TABLE visual_test_runs (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  task_id UUID REFERENCES tasks(id),\n  \n  -- Test configuration\n  app_url TEXT NOT NULL,\n  test_goals TEXT[] NOT NULL,\n  \n  -- Results\n  status VARCHAR(50),  -- running, passed, failed\n  pass_rate DECIMAL(5,2),\n  results JSONB,\n  \n  -- Artifacts\n  screenshots TEXT[],  -- Base64 or S3 URLs\n  \n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  completed_at TIMESTAMPTZ\n);\n\n-- Index for task lookup\nCREATE INDEX idx_visual_test_runs_task_id ON visual_test_runs(task_id);\n```\n\n---\n\n## API Endpoint\n\n```typescript\n// POST /api/tasks/:id/visual-test\nrouter.post(\"/api/tasks/:id/visual-test\", async (req, res) => {\n  const { id } = req.params;\n  const { appUrl, testGoals } = req.body;\n  \n  const task = await db.getTask(id);\n  if (!task) return res.status(404).json({ error: \"Task not found\" });\n  \n  const foreman = new Foreman();\n  const results = await foreman.runVisualTests(appUrl, testGoals);\n  \n  // Store results\n  await db.createVisualTestRun({\n    taskId: id,\n    appUrl,\n    testGoals,\n    status: results.passRate === 100 ? \"passed\" : \"failed\",\n    passRate: results.passRate,\n    results: results.results,\n    screenshots: results.results.flatMap(r => r.screenshots),\n  });\n  \n  return res.json(results);\n});\n```\n\n---\n\n## Acceptance Criteria\n\n- [ ] ComputerUseAgent class implementation\n- [ ] Playwright browser integration  \n- [ ] CUA loop with action execution (all 8 action types)\n- [ ] Safety check handling (malicious, irrelevant, sensitive)\n- [ ] Integration with Foreman\n- [ ] Screenshot capture and storage\n- [ ] API endpoint for visual test runs\n- [ ] Configuration via environment variables\n- [ ] Docker setup for sandboxed execution\n- [ ] Documentation for visual testing workflow\n\n## Complexity\n\n**L** - New capability, external API, browser automation\n\n## Limitations (from OpenAI)\n\n- Model is 38.1% accurate on OSWorld (not fully reliable)\n- Best for browser-based tasks\n- May make mistakes, needs human oversight\n- Requires explicit acknowledgment for safety checks\n\n## References\n\n- [OpenAI Computer Use Documentation](https://platform.openai.com/docs/guides/computer-use)\n- [OpenAI CUA Sample App](https://github.com/openai/openai-cua-sample-app)\n- [Playwright Documentation](https://playwright.dev/docs/intro)\n",
    "number": 245,
    "title": "feat: Integrate Computer Use Agent for Visual Testing",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/245"
  },
  {
    "body": "## Summary\n\nIntegrate OpenAI's Prompt Optimizer to automatically improve AutoDev agent prompts based on task success/failure data and annotations.\n\n## Background\n\nFrom OpenAI's Prompt Optimizer documentation:\n> \"The prompt optimizer takes your generated output, custom annotation columns, and graders into consideration to construct an improved prompt.\"\n\nAutoDev has extensive task history with success/failure signals that can be used to optimize prompts for Planner, Coder, Fixer, and Reviewer agents.\n\n## The Evaluation Flywheel\n\nOpenAI recommends a continuous improvement cycle:\n\n```\n    ┌─────────────┐\n    │   ANALYZE   │ ← Manual review, annotation\n    │  (Open/Axial│   identify failure modes\n    │   Coding)   │\n    └──────┬──────┘\n           │\n           ↓\n    ┌─────────────┐\n    │   MEASURE   │ ← Automated graders\n    │  (Graders   │   quantify failures\n    │   Evals)    │\n    └──────┬──────┘\n           │\n           ↓\n    ┌─────────────┐\n    │   IMPROVE   │ ← Prompt optimization\n    │  (Optimize  │   targeted improvements\n    │   Prompt)   │\n    └──────┬──────┘\n           │\n           └────→ Repeat\n```\n\n## Requirements\n\n### Data Collection for Optimization\n\n```typescript\ninterface PromptOptimizationDataset {\n  promptId: string;         // e.g., \"planner\", \"coder\", \"fixer\"\n  version: number;\n  \n  rows: {\n    // Input\n    input: Record<string, string>;  // Variables passed to prompt\n    \n    // Output\n    output: string;                 // Model response\n    \n    // Annotations\n    rating: \"good\" | \"bad\";\n    outputFeedback?: string;        // Text critique\n    \n    // Custom annotations (axial codes)\n    failureMode?: string;           // e.g., \"wrong_files\", \"incomplete_diff\"\n    \n    // Ground truth (for graders)\n    expectedFiles?: string[];\n    testsPassed?: boolean;\n    prMerged?: boolean;\n  }[];\n}\n```\n\n### Failure Mode Taxonomy (Axial Codes)\n\n| Agent | Failure Modes |\n|-------|---------------|\n| **Planner** | wrong_files, missing_acceptance_criteria, wrong_complexity, incomplete_plan |\n| **Coder** | syntax_error, incomplete_diff, wrong_approach, missing_imports |\n| **Fixer** | same_error_repeated, introduced_new_bug, wrong_fix_location |\n| **Reviewer** | false_positive, false_negative, unclear_feedback |\n\n### Graders for AutoDev\n\n```typescript\n// Grader types we need\ninterface AutoDevGraders {\n  // String check\n  filesMatch: {\n    type: \"string_check\";\n    compare: \"targetFiles\";\n    reference: \"expectedFiles\";\n  };\n  \n  // Python code execution\n  diffValid: {\n    type: \"python\";\n    code: \"return parse_diff(output) is not None\";\n  };\n  \n  // LLM grader (label)\n  codeQuality: {\n    type: \"label_model\";\n    labels: [\"excellent\", \"good\", \"needs_improvement\", \"poor\"];\n    prompt: \"Evaluate the code quality...\";\n  };\n  \n  // LLM grader (score)\n  planCompleteness: {\n    type: \"score_model\";\n    range: [1, 5];\n    prompt: \"Rate how complete this plan is...\";\n  };\n}\n```\n\n### Integration with OpenAI Platform\n\n```typescript\n// src/core/prompt-optimization/optimizer.ts\nexport class PromptOptimizer {\n  // Export dataset for OpenAI Platform\n  async exportDataset(\n    promptId: string,\n    options: { minRows?: number; includeAnnotations?: boolean }\n  ): Promise<DatasetExport>;\n  \n  // Import optimized prompt from Platform\n  async importOptimizedPrompt(\n    promptId: string,\n    newVersion: string\n  ): Promise<void>;\n  \n  // Track prompt versions\n  async listPromptVersions(promptId: string): Promise<PromptVersion[]>;\n  \n  // A/B test prompts\n  async startABTest(\n    promptId: string,\n    versionA: string,\n    versionB: string,\n    trafficSplit: number\n  ): Promise<ABTest>;\n}\n```\n\n### Workflow\n\n1. **Collect data** from task executions (input, output, success/failure)\n2. **Annotate** failures with failure modes (manual or automated)\n3. **Export** dataset to OpenAI Platform\n4. **Run** prompt optimizer on Platform\n5. **Import** optimized prompt back to AutoDev\n6. **A/B test** old vs new prompt\n7. **Deploy** if metrics improve\n\n### Database Schema\n\n```sql\nCREATE TABLE prompt_versions (\n  id UUID PRIMARY KEY,\n  prompt_id VARCHAR(50) NOT NULL,  -- planner, coder, fixer, reviewer\n  version INTEGER NOT NULL,\n  content TEXT NOT NULL,\n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  is_active BOOLEAN DEFAULT false,\n  \n  -- Performance metrics\n  tasks_executed INTEGER DEFAULT 0,\n  success_rate DECIMAL(5,2),\n  avg_tokens INTEGER,\n  \n  UNIQUE(prompt_id, version)\n);\n\nCREATE TABLE prompt_optimization_data (\n  id UUID PRIMARY KEY,\n  prompt_id VARCHAR(50) NOT NULL,\n  task_id UUID REFERENCES tasks(id),\n  \n  -- Input/Output\n  input_variables JSONB,\n  output TEXT,\n  \n  -- Annotations\n  rating VARCHAR(10),\n  output_feedback TEXT,\n  failure_mode VARCHAR(50),\n  \n  -- Grader results\n  grader_results JSONB,\n  \n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n### API Endpoints\n\n```\nGET  /api/prompts                         - List all prompts with versions\nGET  /api/prompts/:id/versions            - List versions for prompt\nPOST /api/prompts/:id/export              - Export dataset for optimization\nPOST /api/prompts/:id/import              - Import optimized prompt\nPOST /api/prompts/:id/ab-test             - Start A/B test\nGET  /api/prompts/:id/ab-test/results     - Get A/B test results\nPOST /api/prompts/:id/deploy/:version     - Deploy specific version\n```\n\n### Configuration\n\n```bash\nENABLE_PROMPT_OPTIMIZATION=true\nPROMPT_AB_TEST_TRAFFIC_SPLIT=0.5    # 50% to each version\nPROMPT_MIN_SAMPLES_FOR_OPTIMIZATION=50\n```\n\n## Acceptance Criteria\n- [ ] Prompt version tracking in database\n- [ ] Data collection from task executions\n- [ ] Annotation support for failure modes\n- [ ] Dataset export for OpenAI Platform\n- [ ] Prompt import from Platform\n- [ ] A/B testing infrastructure\n- [ ] API endpoints for prompt management\n- [ ] Grader definitions for each agent\n- [ ] Documentation for optimization workflow\n\n## Complexity\nM - Multiple components, Platform integration\n\n## References\n- OpenAI Prompt Optimizer documentation\n- OpenAI Datasets documentation\n- OpenAI Cookbook: Building resilient prompts with evals",
    "number": 244,
    "title": "feat: Integrate OpenAI Prompt Optimizer for Agent Prompts",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/244"
  },
  {
    "body": "## Summary\n\nIntegrate OpenAI's Flex Processing to get 50% cost savings on non-urgent tasks like evals, Knowledge Graph sync, and overnight batch processing.\n\n## Background\n\nFrom OpenAI's Flex Processing documentation:\n> \"Flex processing provides lower costs for Responses or Chat Completions requests in exchange for slower response times and occasional resource unavailability. Tokens are priced at Batch API rates.\"\n\n**Key characteristics:**\n- 50% cost discount (same as Batch API)\n- Synchronous API (unlike Batch which is async)\n- May return `429 Resource Unavailable` errors\n- Slower response times, may need longer timeouts\n\n## Requirements\n\n### Use Cases for Flex Processing\n\n| Use Case | Why Flex? |\n|----------|-----------|\n| Running evals | Not time-sensitive, can retry on failure |\n| Knowledge Graph sync | Background task, overnight is fine |\n| Distillation data collection | Bulk processing, cost matters |\n| Re-processing failed tasks | Lower priority than new tasks |\n| Pre-computing embeddings | Batch operation, latency not critical |\n\n### Implementation\n\n```typescript\n// src/integrations/openai-flex.ts\nexport class OpenAIFlexClient {\n  private client: OpenAI;\n  \n  constructor() {\n    this.client = new OpenAI({\n      timeout: 15 * 60 * 1000,  // 15 minutes (flex needs longer timeout)\n    });\n  }\n  \n  async complete(params: CompletionParams): Promise<CompletionResponse> {\n    try {\n      return await this.client.responses.create({\n        ...params,\n        service_tier: \"flex\",\n      });\n    } catch (error) {\n      if (error.status === 429 && error.code === \"resource_unavailable\") {\n        // Retry with standard processing or exponential backoff\n        return this.handleResourceUnavailable(params, error);\n      }\n      throw error;\n    }\n  }\n  \n  private async handleResourceUnavailable(\n    params: CompletionParams,\n    error: Error\n  ): Promise<CompletionResponse> {\n    // Option 1: Retry with exponential backoff\n    // Option 2: Fall back to standard processing\n    // Configurable per use case\n  }\n}\n```\n\n### Integration with LLM Router\n\n```typescript\n// src/integrations/llm.ts\nexport interface CompletionParams {\n  // ... existing params\n  serviceTier?: \"auto\" | \"flex\";  // New parameter\n}\n\n// Route to flex for eligible requests\nif (params.serviceTier === \"flex\") {\n  return getOpenAIFlexClient().complete(params);\n}\n```\n\n### Fallback Strategy\n\n```typescript\ninterface FlexConfig {\n  enableFlex: boolean;\n  maxRetries: number;\n  retryDelayMs: number;\n  fallbackToStandard: boolean;  // If flex unavailable, use standard\n}\n\n// Default config\nconst DEFAULT_FLEX_CONFIG: FlexConfig = {\n  enableFlex: true,\n  maxRetries: 3,\n  retryDelayMs: 60000,  // 1 minute between retries\n  fallbackToStandard: true,\n};\n```\n\n### Cost Tracking\n\nTrack flex vs standard usage for cost analysis:\n```typescript\ninterface FlexMetrics {\n  flexRequests: number;\n  flexTokens: number;\n  standardFallbacks: number;\n  resourceUnavailableErrors: number;\n  estimatedSavings: number;  // USD saved vs standard\n}\n```\n\n### Configuration\n\n```bash\nENABLE_FLEX_PROCESSING=true\nFLEX_TIMEOUT_MS=900000            # 15 minutes\nFLEX_MAX_RETRIES=3\nFLEX_FALLBACK_TO_STANDARD=true\nFLEX_ELIGIBLE_OPERATIONS=evals,kg_sync,distillation,embeddings\n```\n\n### Usage in Orchestrator\n\n```typescript\n// For eval runs - use flex\nconst evalResult = await llmClient.complete({\n  ...params,\n  serviceTier: \"flex\",\n});\n\n// For real-time task processing - use standard\nconst coderResult = await llmClient.complete({\n  ...params,\n  serviceTier: \"auto\",  // Default, uses standard\n});\n```\n\n## Cost Savings Estimate\n\n| Operation | Volume/Month | Standard Cost | Flex Cost | Savings |\n|-----------|--------------|---------------|-----------|---------|\n| Eval runs | 1000 | $100 | $50 | $50 |\n| KG sync | 500 | $50 | $25 | $25 |\n| Embeddings | 10000 | $20 | $10 | $10 |\n| **Total** | | **$170** | **$85** | **$85** |\n\n## Acceptance Criteria\n- [ ] OpenAIFlexClient implementation\n- [ ] Integration with LLM router (serviceTier param)\n- [ ] Resource unavailable error handling\n- [ ] Exponential backoff retry logic\n- [ ] Fallback to standard processing option\n- [ ] Cost tracking and metrics\n- [ ] Configuration via environment variables\n- [ ] Unit tests for flex client\n- [ ] Documentation for when to use flex\n\n## Complexity\nS - Wrapper around existing API with retry logic\n\n## References\n- OpenAI Flex Processing documentation\n- OpenAI Cost Optimization guide\n- Supported models: Check pricing page for flex-eligible models",
    "number": 243,
    "title": "feat: Implement Flex Processing for Low-Priority Tasks",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/243"
  },
  {
    "body": "## Summary\n\nIntegrate OpenAI's Batch API to process non-urgent tasks at 50% lower cost with higher rate limits.\n\n## Background\n\nFrom OpenAI's Batch API documentation:\n> \"Batch API offers 50% cost discount compared to synchronous APIs, substantially more headroom on rate limits, and 24-hour completion time.\"\n\nAutoDev use cases that fit batch processing:\n- Overnight processing of backlog issues\n- Running evals on historical tasks\n- Pre-computing embeddings for Knowledge Graph\n- Bulk re-processing of failed tasks\n\n## Requirements\n\n### Batch Job Types\n\n| Job Type | Description | Priority |\n|----------|-------------|----------|\n| `task_processing` | Process multiple issues overnight | High |\n| `eval_run` | Run evals on batch of tasks | Medium |\n| `embedding_compute` | Compute embeddings for files | Medium |\n| `reprocess_failed` | Retry failed tasks in batch | Low |\n\n### Batch Request Format\n\n```typescript\ninterface BatchRequest {\n  custom_id: string;      // Task ID or unique identifier\n  method: \"POST\";\n  url: \"/v1/responses\" | \"/v1/chat/completions\";\n  body: {\n    model: string;\n    messages?: Message[];\n    input?: string;\n    // ... other params\n  };\n}\n\n// Example batch input file (JSONL)\n{\"custom_id\": \"task-123\", \"method\": \"POST\", \"url\": \"/v1/responses\", \"body\": {\"model\": \"gpt-5.2\", \"input\": \"...\"}}\n{\"custom_id\": \"task-456\", \"method\": \"POST\", \"url\": \"/v1/responses\", \"body\": {\"model\": \"gpt-5.2\", \"input\": \"...\"}}\n```\n\n### Implementation\n\n```typescript\n// src/integrations/openai-batch.ts\nexport class OpenAIBatchClient {\n  private client: OpenAI;\n  \n  // Create batch input file\n  async createBatchFile(requests: BatchRequest[]): Promise<string> {\n    const jsonl = requests.map(r => JSON.stringify(r)).join(\"\\n\");\n    const file = await this.client.files.create({\n      file: Buffer.from(jsonl),\n      purpose: \"batch\",\n    });\n    return file.id;\n  }\n  \n  // Submit batch job\n  async submitBatch(\n    inputFileId: string,\n    endpoint: \"/v1/responses\" | \"/v1/chat/completions\",\n    metadata?: Record<string, string>\n  ): Promise<Batch> {\n    return this.client.batches.create({\n      input_file_id: inputFileId,\n      endpoint,\n      completion_window: \"24h\",\n      metadata,\n    });\n  }\n  \n  // Check batch status\n  async getBatchStatus(batchId: string): Promise<Batch>;\n  \n  // Retrieve results\n  async getBatchResults(batchId: string): Promise<BatchResult[]>;\n  \n  // Cancel batch\n  async cancelBatch(batchId: string): Promise<void>;\n  \n  // List all batches\n  async listBatches(limit?: number): Promise<Batch[]>;\n}\n```\n\n### Batch Job Runner\n\n```typescript\n// src/core/batch-job-runner.ts\nexport class BatchJobRunner {\n  async createTaskBatch(\n    taskIds: string[],\n    agentType: \"planner\" | \"coder\" | \"fixer\" | \"reviewer\"\n  ): Promise<string>;\n  \n  async processCompletedBatch(batchId: string): Promise<void>;\n  \n  // Schedule overnight batch processing\n  async scheduleOvernightBatch(\n    repo: string,\n    options: { maxTasks?: number; priority?: string }\n  ): Promise<string>;\n}\n```\n\n### Database Schema\n\n```sql\nCREATE TABLE batch_jobs (\n  id UUID PRIMARY KEY,\n  openai_batch_id VARCHAR(100) UNIQUE,\n  job_type VARCHAR(50) NOT NULL,\n  \n  -- Status\n  status VARCHAR(50) NOT NULL,  -- pending, submitted, in_progress, completed, failed\n  input_file_id VARCHAR(100),\n  output_file_id VARCHAR(100),\n  error_file_id VARCHAR(100),\n  \n  -- Counts\n  total_requests INTEGER,\n  completed_requests INTEGER DEFAULT 0,\n  failed_requests INTEGER DEFAULT 0,\n  \n  -- Timing\n  submitted_at TIMESTAMPTZ,\n  completed_at TIMESTAMPTZ,\n  expires_at TIMESTAMPTZ,\n  \n  -- Metadata\n  metadata JSONB,\n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE TABLE batch_job_tasks (\n  id UUID PRIMARY KEY,\n  batch_job_id UUID REFERENCES batch_jobs(id),\n  task_id UUID REFERENCES tasks(id),\n  custom_id VARCHAR(100) NOT NULL,\n  status VARCHAR(50),\n  result JSONB,\n  error JSONB,\n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n### API Endpoints\n\n```\nPOST /api/batch/create           - Create new batch job\nGET  /api/batch/:id              - Get batch status\nGET  /api/batch/:id/results      - Get batch results\nPOST /api/batch/:id/cancel       - Cancel batch\nGET  /api/batch                  - List all batches\nPOST /api/batch/schedule         - Schedule overnight batch\n```\n\n### Webhook for Batch Completion\n\n```typescript\n// Poll for completion or use webhook\nasync function pollBatchCompletion(batchId: string): Promise<void> {\n  const batch = await batchClient.getBatchStatus(batchId);\n  \n  if (batch.status === \"completed\") {\n    await processBatchResults(batchId);\n  } else if (batch.status === \"failed\" || batch.status === \"expired\") {\n    await handleBatchFailure(batchId);\n  } else {\n    // Schedule next poll\n    await scheduleCheck(batchId, 60000); // 1 minute\n  }\n}\n```\n\n### Configuration\n\n```bash\nENABLE_BATCH_API=true\nBATCH_AUTO_OVERNIGHT=true         # Auto-batch pending tasks overnight\nBATCH_OVERNIGHT_HOUR=2            # 2 AM local time\nBATCH_MAX_REQUESTS=1000           # Max requests per batch\nBATCH_POLL_INTERVAL_MS=60000      # 1 minute\n```\n\n## Cost Savings Estimate\n\n| Scenario | Sync Cost | Batch Cost | Savings |\n|----------|-----------|------------|---------|\n| 100 XS tasks | $10.00 | $5.00 | 50% |\n| Eval run (500 examples) | $50.00 | $25.00 | 50% |\n| Knowledge Graph sync | $20.00 | $10.00 | 50% |\n\n## Acceptance Criteria\n- [ ] OpenAIBatchClient implementation\n- [ ] Batch job database schema and migrations\n- [ ] Create batch from pending tasks\n- [ ] Poll and process completed batches\n- [ ] Error handling for failed/expired batches\n- [ ] API endpoints for batch management\n- [ ] Overnight scheduling\n- [ ] Integration with existing job runner\n- [ ] Unit tests for batch operations\n\n## Complexity\nM - External API integration, async processing\n\n## References\n- OpenAI Batch API documentation\n- OpenAI Cookbook: Batch API examples\n- Limits: 50,000 requests/batch, 200MB input file",
    "number": 242,
    "title": "feat: Integrate OpenAI Batch API for Async Processing",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/242"
  },
  {
    "body": "## Summary\n\nImplement a distillation pipeline to train smaller, faster models from successful AutoDev outputs, reducing cost and latency while maintaining quality.\n\n## Background\n\nFrom OpenAI's Model Optimization track:\n> \"Distillation is a way to transfer a stronger model's behavior to a smaller 'student' model, maintaining performance while improving speed and cost.\"\n\nAutoDev generates successful diffs daily with Opus/GPT-5.2. This is valuable training data that could be used to fine-tune smaller models (like Grok Fast) to handle XS-low tasks with equal quality at 10-50x lower cost.\n\n## Requirements\n\n### Distillation Workflow\n\n```\n1. Collect successful task outputs (input → output pairs)\n2. Filter for high-quality examples (tests passed, PR merged)\n3. Create eval set to measure baseline performance\n4. Fine-tune smaller model on collected examples\n5. Evaluate fine-tuned model against baseline\n6. Deploy if quality threshold met\n```\n\n### Data Collection\n\n```typescript\ninterface DistillationExample {\n  // Input\n  issueTitle: string;\n  issueBody: string;\n  targetFiles: string[];\n  fileContents: Record<string, string>;\n  plan: string;\n  \n  // Output (from successful task)\n  diff: string;\n  commitMessage: string;\n  \n  // Metadata\n  sourceModel: string;      // e.g., \"claude-opus-4-5\"\n  complexity: string;\n  effort: string;\n  tokensUsed: number;\n  \n  // Quality signals\n  testsPassed: boolean;\n  reviewApproved: boolean;\n  prMerged: boolean;\n  humanEditsRequired: number; // Lines changed by human\n}\n```\n\n### Database Schema\n\n```sql\nCREATE TABLE distillation_examples (\n  id UUID PRIMARY KEY,\n  task_id UUID REFERENCES tasks(id),\n  \n  -- Input\n  issue_title TEXT NOT NULL,\n  issue_body TEXT,\n  target_files TEXT[],\n  file_contents JSONB,\n  plan TEXT,\n  \n  -- Output\n  diff TEXT NOT NULL,\n  commit_message TEXT,\n  \n  -- Metadata\n  source_model VARCHAR(100),\n  complexity VARCHAR(10),\n  effort VARCHAR(20),\n  tokens_used INTEGER,\n  \n  -- Quality signals\n  tests_passed BOOLEAN,\n  review_approved BOOLEAN,\n  pr_merged BOOLEAN,\n  human_edits INTEGER DEFAULT 0,\n  \n  -- Distillation status\n  included_in_training BOOLEAN DEFAULT false,\n  training_job_id VARCHAR(100),\n  \n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE INDEX idx_de_quality ON distillation_examples(tests_passed, review_approved, pr_merged);\nCREATE INDEX idx_de_complexity ON distillation_examples(complexity, effort);\n```\n\n### Quality Filtering\n\nOnly include examples that meet criteria:\n```typescript\nfunction isHighQualityExample(example: DistillationExample): boolean {\n  return (\n    example.testsPassed &&\n    example.reviewApproved &&\n    example.prMerged &&\n    example.humanEditsRequired <= 5 &&  // Minimal human cleanup\n    example.tokensUsed < 10000          // Not overly complex\n  );\n}\n```\n\n### Fine-Tuning Integration\n\n```typescript\n// src/core/distillation/trainer.ts\nexport class DistillationTrainer {\n  async collectExamples(\n    minExamples: number = 50,\n    targetComplexity?: string\n  ): Promise<DistillationExample[]>;\n  \n  async exportToJSONL(\n    examples: DistillationExample[],\n    outputPath: string\n  ): Promise<void>;\n  \n  async startFineTuning(\n    baseModel: string,           // e.g., \"gpt-4o-mini\"\n    trainingFile: string,\n    validationFile?: string\n  ): Promise<string>;            // Returns job ID\n  \n  async evaluateModel(\n    modelId: string,\n    evalSet: DistillationExample[]\n  ): Promise<EvalResults>;\n}\n```\n\n### Target Models for Distillation\n\n| Source Model | Target Model | Use Case |\n|--------------|--------------|----------|\n| claude-opus-4-5 | gpt-4o-mini | XS-low tasks |\n| gpt-5.2 | grok-code-fast | Simple fixes |\n| Multi-agent consensus | Single fine-tuned | Reduce multi-agent cost |\n\n### API Endpoints\n\n```\nGET  /api/distillation/examples       - List collected examples\nPOST /api/distillation/collect        - Trigger collection from recent tasks\nPOST /api/distillation/train          - Start fine-tuning job\nGET  /api/distillation/jobs/:id       - Get training job status\nPOST /api/distillation/evaluate       - Evaluate fine-tuned model\nPOST /api/distillation/deploy         - Deploy model to production tier\n```\n\n### Configuration\n\n```bash\nENABLE_DISTILLATION=true\nDISTILLATION_MIN_EXAMPLES=50\nDISTILLATION_QUALITY_THRESHOLD=0.9    # 90% of baseline performance\nDISTILLATION_AUTO_COLLECT=true        # Auto-collect from successful tasks\n```\n\n## Acceptance Criteria\n- [ ] DistillationExample schema and database migration\n- [ ] Auto-collection from successful tasks (on PR merge)\n- [ ] Quality filtering logic\n- [ ] Export to JSONL for OpenAI fine-tuning\n- [ ] Fine-tuning job management\n- [ ] Evaluation against baseline\n- [ ] API endpoints for management\n- [ ] Integration with model-selection.ts for deployment\n- [ ] Documentation for distillation workflow\n\n## Complexity\nM - Multiple components, external API integration\n\n## References\n- OpenAI Fine-tuning API\n- OpenAI Model Optimization track: \"Distillation\"\n- OpenAI Cookbook: Distillation examples",
    "number": 241,
    "title": "feat: Implement Model Distillation Pipeline",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/241"
  },
  {
    "body": "## Summary\n\nImplement prompt caching to reduce latency and cost when sending repeated context (system prompts, repo context, file contents) across multiple LLM calls.\n\n## Background\n\nFrom OpenAI's production track:\n> \"Prompt caching: you can use prompt caching to improve latency and reduce costs for cached tokens (series of tokens that have already been seen by the model)\"\n\nAutoDev sends similar context repeatedly:\n- System prompts (planner, coder, fixer, reviewer)\n- Repository structure and conventions\n- File contents for target files\n- Previous conversation context\n\n## Requirements\n\n### What to Cache\n\n| Content Type | Cache Duration | Benefit |\n|--------------|----------------|---------|\n| System prompts | 24h | Reused every call |\n| Repo context (structure, conventions) | 1h | Same across tasks |\n| File contents | 5m | Same within task |\n| Prompt templates | 24h | Static |\n\n### Cache Key Strategy\n\n```typescript\ninterface CacheKey {\n  type: \"system\" | \"repo\" | \"file\" | \"template\";\n  identifier: string;  // prompt name, repo, file path\n  contentHash: string; // SHA256 of content\n}\n```\n\n### Prompt Structure Optimization\n\nReorder prompts to maximize cache hits:\n\n```\nSTATIC (always cached)       → System prompt, base instructions\nSEMI-STATIC (per repo)       → Repo conventions, directory structure\nTASK-SPECIFIC (per task)     → Target file contents, issue description\nDYNAMIC (never cached)       → Attempt number, error messages\n```\n\n### Configuration\n\n```bash\nENABLE_PROMPT_CACHE=true\nPROMPT_CACHE_TTL_MS=3600000\nPROMPT_CACHE_BACKEND=memory  # memory | redis\n```\n\n## Acceptance Criteria\n- [ ] PromptCache interface and implementation\n- [ ] Cache key generation with content hashing\n- [ ] TTL-based expiration\n- [ ] Prompt structure optimization\n- [ ] Cache metrics tracking\n- [ ] Integration with LLM client\n- [ ] Unit tests\n\n## Complexity\nS - Straightforward caching logic\n\n## References\n- OpenAI prompt caching docs\n- OpenAI production track: \"Cost & latency optimization\"",
    "number": 240,
    "title": "feat: Implement Prompt Caching for Repeated Context",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/240"
  },
  {
    "body": "## Summary\n\nImplement input guardrails to validate GitHub issues before processing, preventing wasted compute on malformed, unsafe, or unclear requests.\n\n## Background\n\nFrom OpenAI's production track:\n> \"Input guardrails prevent unwanted inputs from being processed. In a production environment, ideally you would have both types of guardrails.\"\n\nAutoDev currently has strong **output** guardrails (MAX_DIFF_LINES, blocked paths, complexity limits) but minimal **input** validation beyond checking for the `auto-dev` label.\n\n## Requirements\n\n### Validation Checks\n\n#### 1. Content Moderation\nUse OpenAI Moderations API (free) to check issue content:\n```typescript\nconst moderation = await openai.moderations.create({\n  model: \"omni-moderation-latest\",\n  input: issue.title + \" \" + issue.body,\n});\n\nif (moderation.results[0].flagged) {\n  return reject(\"Issue flagged by content moderation\");\n}\n```\n\n#### 2. Acceptance Criteria Check\nVerify issue has clear requirements:\n```typescript\ninterface IssueQualityCheck {\n  hasAcceptanceCriteria: boolean;  // Contains \"- [ ]\" or \"acceptance criteria\"\n  hasCodeReferences: boolean;       // Contains file paths or function names\n  estimatedClarity: number;         // 0-100 from LLM\n  missingInfo: string[];            // What's unclear\n}\n```\n\n#### 3. Security-Sensitive Detection\nFlag issues that modify sensitive areas:\n- Authentication/authorization changes\n- Environment variables or secrets\n- CI/CD workflows\n- Database migrations\n- Payment/billing code\n\n#### 4. Scope Validation\nReject issues that are too vague:\n- \"Fix the bug\" (which bug?)\n- \"Improve performance\" (where?)\n- \"Update the code\" (what code?)\n\n### Guardrail Response Actions\n\n```typescript\ntype GuardrailAction = \n  | \"pass\"           // Proceed with processing\n  | \"warn\"           // Proceed but flag for human review\n  | \"clarify\"        // Ask for more info (comment on issue)\n  | \"reject\";        // Do not process, add label\n\ninterface GuardrailResult {\n  action: GuardrailAction;\n  reason: string;\n  details: {\n    moderationFlags?: string[];\n    missingInfo?: string[];\n    securityConcerns?: string[];\n    clarityScore?: number;\n  };\n}\n```\n\n### Implementation\n\n```typescript\n// src/core/guardrails/input-guardrails.ts\nexport class InputGuardrails {\n  async validate(issue: GitHubIssue): Promise<GuardrailResult> {\n    // 1. Content moderation\n    const modResult = await this.checkModeration(issue);\n    if (modResult.action === \"reject\") return modResult;\n    \n    // 2. Acceptance criteria\n    const criteriaResult = await this.checkAcceptanceCriteria(issue);\n    if (criteriaResult.action === \"clarify\") return criteriaResult;\n    \n    // 3. Security check\n    const securityResult = await this.checkSecuritySensitive(issue);\n    if (securityResult.action !== \"pass\") return securityResult;\n    \n    // 4. Clarity check\n    const clarityResult = await this.checkClarity(issue);\n    return clarityResult;\n  }\n  \n  private async checkModeration(issue: GitHubIssue): Promise<GuardrailResult>;\n  private async checkAcceptanceCriteria(issue: GitHubIssue): Promise<GuardrailResult>;\n  private async checkSecuritySensitive(issue: GitHubIssue): Promise<GuardrailResult>;\n  private async checkClarity(issue: GitHubIssue): Promise<GuardrailResult>;\n}\n```\n\n### Auto-Comment for Clarification\n\nWhen action is \"clarify\", post a comment:\n```markdown\n👋 AutoDev needs more information to process this issue:\n\n**Missing details:**\n- [ ] Specific file(s) to modify\n- [ ] Expected behavior after the change\n- [ ] How to verify the fix works\n\nPlease update the issue description and remove the `needs-info` label when ready.\n```\n\n### Configuration\n\n```bash\n# Enable/disable guardrails\nINPUT_GUARDRAILS_ENABLED=true\n\n# Thresholds\nMIN_CLARITY_SCORE=60\nREQUIRE_ACCEPTANCE_CRITERIA=true\n\n# Security\nSECURITY_SENSITIVE_PATHS=src/auth,src/payment,lib/secrets\nAUTO_REJECT_SECURITY=false  # false = warn only\n```\n\n## Acceptance Criteria\n- [ ] InputGuardrails class implemented\n- [ ] Content moderation integration (Moderations API)\n- [ ] Acceptance criteria detection\n- [ ] Security-sensitive path detection\n- [ ] Clarity scoring with LLM\n- [ ] Auto-comment for clarification requests\n- [ ] Configuration via environment variables\n- [ ] Unit tests for each guardrail type\n- [ ] Integration with webhook handler\n\n## Complexity\nS - Well-defined scope, mostly validation logic\n\n## References\n- OpenAI Moderations API (free)\n- OpenAI production track: \"Building guardrails\"",
    "number": 239,
    "title": "feat: Add Input Guardrails for Issue Validation",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/239"
  },
  {
    "body": "## Summary\n\nImplement a comprehensive evaluation framework to measure and track AutoDev task quality over time, enabling data-driven improvements and model comparisons.\n\n## Background\n\nFrom OpenAI's production track:\n> \"Evals are how you measure and improve your AI app's behavior. They help verify correctness, enforce guardrails, and track quality over time so you can ship with confidence.\"\n\nAutoDev currently lacks systematic evaluation of:\n- Code quality of generated diffs\n- Success rates by complexity/effort\n- Model performance comparisons\n- Fix loop frequency\n\n## Requirements\n\n### Metrics to Track\n\n```typescript\ninterface TaskEvalMetrics {\n  taskId: string;\n  \n  // Success metrics\n  succeeded: boolean;\n  attemptsRequired: number;\n  fixLoopsTriggered: number;\n  \n  // Quality metrics\n  diffLinesGenerated: number;\n  diffLinesNeeded: number;  // After human cleanup\n  codeQualityScore: number; // 0-100, from grader\n  \n  // Efficiency metrics  \n  totalTokensUsed: number;\n  totalCostUsd: number;\n  totalDurationMs: number;\n  \n  // Model info\n  modelsUsed: string[];\n  finalModel: string;\n  \n  // Context\n  complexity: \"XS\" | \"S\" | \"M\" | \"L\" | \"XL\";\n  effort: \"low\" | \"medium\" | \"high\";\n  repo: string;\n}\n```\n\n### Eval Types\n\n1. **Correctness Evals**\n   - Did tests pass?\n   - Did review approve?\n   - Was PR merged without changes?\n\n2. **Quality Evals**\n   - Diff size vs optimal (human benchmark)\n   - Code style compliance\n   - No unnecessary changes\n\n3. **Efficiency Evals**\n   - Tokens per successful task\n   - Cost per successful task\n   - Time to completion\n\n### Database Schema\n\n```sql\nCREATE TABLE task_evals (\n  id UUID PRIMARY KEY,\n  task_id UUID REFERENCES tasks(id),\n  \n  -- Success\n  succeeded BOOLEAN NOT NULL,\n  attempts_required INTEGER,\n  fix_loops INTEGER,\n  \n  -- Quality\n  diff_lines_generated INTEGER,\n  diff_lines_final INTEGER,\n  code_quality_score DECIMAL(5,2),\n  \n  -- Efficiency\n  total_tokens INTEGER,\n  total_cost_usd DECIMAL(10,4),\n  total_duration_ms INTEGER,\n  \n  -- Context\n  models_used TEXT[],\n  final_model VARCHAR(100),\n  complexity VARCHAR(10),\n  effort VARCHAR(20),\n  \n  evaluated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE TABLE eval_benchmarks (\n  id UUID PRIMARY KEY,\n  name VARCHAR(255) NOT NULL,\n  description TEXT,\n  metric VARCHAR(50) NOT NULL,\n  threshold DECIMAL(10,4),\n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n### API Endpoints\n\n```\nGET  /api/evals/tasks/:taskId     - Get eval for specific task\nGET  /api/evals/summary           - Aggregated metrics\nGET  /api/evals/by-model          - Compare model performance\nGET  /api/evals/by-complexity     - Metrics by task complexity\nGET  /api/evals/trends            - Performance over time\nPOST /api/evals/benchmark         - Create/run benchmark\n```\n\n### Dashboard Integration\n\nAdd to autodev-dashboard:\n- Success rate chart (by day, week, month)\n- Model comparison table\n- Cost per task trend\n- Fix loop frequency\n\n## Implementation\n\n1. Create `src/core/evals/` directory\n2. Implement `EvalCollector` - gathers metrics during task execution\n3. Implement `EvalAnalyzer` - computes aggregates and trends\n4. Add database migration for eval tables\n5. Add API endpoints to router\n6. Create dashboard components\n\n## Acceptance Criteria\n- [ ] TaskEvalMetrics collected for every completed task\n- [ ] Database schema and migrations\n- [ ] API endpoints for querying evals\n- [ ] Summary endpoint with aggregates\n- [ ] Model comparison endpoint\n- [ ] Basic dashboard visualization\n- [ ] Unit tests for eval collection\n\n## Complexity\nM - Multiple components, dashboard integration\n\n## References\n- OpenAI Evals API pattern\n- OpenAI production track: \"Constructing Evals\"",
    "number": 238,
    "title": "feat: Implement Evals Framework for Task Quality Measurement",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/238"
  },
  {
    "body": "## Summary\n\nImplement automatic Knowledge Graph synchronization when a repository is first processed and on subsequent pushes via webhook.\n\n## Background\n\nThe Knowledge Graph needs to be populated and kept in sync with the actual codebase. This happens at two points:\n1. **Initial Sync**: When AutoDev first processes a repo, extract all entities\n2. **Incremental Sync**: On each push webhook, update only changed files\n\n## Requirements\n\n### Initial Sync (Full Repository)\n\n```typescript\nasync function initialSync(repo: string, branch: string): Promise<SyncResult> {\n  const files = await github.listAllFiles(repo, branch, {\n    extensions: [\".ts\", \".tsx\", \".js\", \".jsx\"],\n    excludePaths: [\"node_modules\", \"dist\", \".git\"],\n  });\n  \n  let totalEntities = 0;\n  for (const batch of chunk(files, 10)) {\n    const contents = await github.getFilesContent(repo, batch);\n    const entities = await entityExtractor.extractBatch(contents);\n    const resolved = await entityResolver.resolveBatch(entities);\n    await temporalTracker.recordBatch(resolved, commitSha);\n    totalEntities += resolved.length;\n  }\n  \n  await db.updateSyncState(repo, { \n    status: \"synced\", \n    lastCommitSha: commitSha,\n    entityCount: totalEntities \n  });\n  \n  return { entitiesExtracted: totalEntities };\n}\n```\n\n### Incremental Sync (On Push)\n\n```typescript\nasync function incrementalSync(\n  repo: string, \n  commitSha: string,\n  changedFiles: string[]\n): Promise<SyncResult> {\n  // Get current entities for changed files\n  const existingEntities = await knowledgeGraph.getEntitiesForFiles(changedFiles);\n  \n  // Extract new entities from changed files\n  const contents = await github.getFilesContent(repo, changedFiles);\n  const newEntities = await entityExtractor.extractBatch(contents);\n  const resolved = await entityResolver.resolveBatch(newEntities);\n  \n  // Detect invalidations\n  const invalidations = await invalidationAgent.detect({\n    oldEntities: existingEntities,\n    newEntities: resolved,\n    commitSha,\n  });\n  \n  // Apply updates\n  await temporalTracker.applyUpdates(resolved, invalidations, commitSha);\n  \n  return {\n    entitiesUpdated: resolved.length,\n    entitiesInvalidated: invalidations.length,\n  };\n}\n```\n\n### Webhook Handler\n\nAdd to `src/router.ts`:\n\n```typescript\n// Handle push events for knowledge graph sync\nrouter.post(\"/webhooks/github\", async (req, res) => {\n  const event = req.headers[\"x-github-event\"];\n  \n  if (event === \"push\") {\n    const { repository, after, commits } = req.body;\n    const changedFiles = commits.flatMap(c => [...c.added, ...c.modified, ...c.removed]);\n    \n    // Queue incremental sync (don't block webhook response)\n    await queue.add(\"knowledge-graph-sync\", {\n      repo: repository.full_name,\n      commitSha: after,\n      changedFiles: [...new Set(changedFiles)],\n    });\n  }\n  \n  // ... existing webhook handling\n});\n```\n\n### Sync State Management\n\nTrack sync progress per repository:\n\n```typescript\ninterface SyncState {\n  repoFullName: string;\n  status: \"pending\" | \"syncing\" | \"synced\" | \"failed\";\n  lastCommitSha: string | null;\n  lastSyncAt: Date | null;\n  entityCount: number;\n  errorMessage?: string;\n}\n```\n\n### API Endpoints\n\n```\nPOST /api/knowledge-graph/sync/:repo     - Trigger full sync\nGET  /api/knowledge-graph/status/:repo   - Get sync status\nGET  /api/knowledge-graph/entities/:repo - List entities for repo\n```\n\n## Acceptance Criteria\n- [ ] Initial full-repo sync working\n- [ ] Incremental sync on push webhook\n- [ ] Sync state tracked in database\n- [ ] API endpoints for manual sync trigger\n- [ ] Batched processing to avoid rate limits\n- [ ] Error handling with retry logic\n- [ ] Progress logging for large repos\n\n## Complexity\nM - Webhook integration + batch processing\n\n## Dependencies\n- #230 Entity Extraction Agent\n- #231 Entity Resolution  \n- #232 Temporal Validity Tracker\n- #233 Invalidation Agent\n- #235 Database Schema",
    "number": 237,
    "title": "feat: Knowledge Graph Sync on Repository Clone/Webhook",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/237"
  },
  {
    "body": "## Summary\n\nIntegrate the Temporal Knowledge Graph with AutoDev's orchestrator to enhance context retrieval for coding and fixing tasks.\n\n## Background\n\nOnce the Knowledge Graph is populated, AutoDev can use it to:\n1. Provide better context to Coder/Fixer agents\n2. Predict impact of changes before applying diffs\n3. Avoid breaking changes by understanding dependencies\n4. Learn from historical patterns with temporal awareness\n\n## Requirements\n\n### Integration Points\n\n#### 1. Pre-Coding Context Enhancement\nBefore CoderAgent runs, query Knowledge Graph for:\n- Target file entities and their dependencies\n- Recent changes to related entities\n- Known patterns for similar changes\n\n```typescript\n// In orchestrator.ts\nasync function enhanceContextWithKnowledgeGraph(task: Task): Promise<EnhancedContext> {\n  const targetEntities = await knowledgeGraph.getEntitiesForFiles(task.targetFiles);\n  const dependencies = await multiHopRetriever.findDependencies(targetEntities);\n  const recentChanges = await temporalTracker.getRecentChanges(targetEntities, 7); // last 7 days\n  \n  return {\n    entities: targetEntities,\n    dependencies,\n    recentChanges,\n    impactRadius: dependencies.length,\n  };\n}\n```\n\n#### 2. Pre-Apply Impact Analysis\nBefore applying a diff, analyze potential impact:\n\n```typescript\nasync function analyzeImpact(diff: string, task: Task): Promise<ImpactAnalysis> {\n  const modifiedEntities = await entityExtractor.extractFromDiff(diff);\n  const impacted = await multiHopRetriever.findImpact(modifiedEntities);\n  \n  return {\n    directChanges: modifiedEntities.length,\n    impactedEntities: impacted.length,\n    riskLevel: calculateRisk(impacted),\n    warnings: generateWarnings(impacted),\n  };\n}\n```\n\n#### 3. Post-Apply Knowledge Update\nAfter successfully applying changes:\n\n```typescript\nasync function updateKnowledgeGraph(task: Task, commitSha: string): Promise<void> {\n  // Extract entities from modified files\n  const newEntities = await entityExtractor.extractFromFiles(task.targetFiles);\n  \n  // Resolve and update\n  const resolved = await entityResolver.resolve(newEntities);\n  \n  // Detect invalidations\n  const invalidations = await invalidationAgent.detect(resolved, commitSha);\n  \n  // Apply updates\n  await temporalTracker.applyUpdates(resolved, invalidations, commitSha);\n}\n```\n\n#### 4. Enhanced Fix Context\nWhen FixerAgent runs, provide:\n- Entity history (what changed recently)\n- Related fixes (temporal learning)\n- Dependency chain that might be affected\n\n### New Orchestrator Methods\n\n```typescript\nclass Orchestrator {\n  // Existing methods...\n  \n  // New Knowledge Graph integration\n  private knowledgeGraph: KnowledgeGraphService;\n  \n  async runCodingWithKnowledge(task: Task): Promise<CoderOutput> {\n    const context = await this.enhanceContextWithKnowledgeGraph(task);\n    const coderOutput = await this.runCoding(task, context);\n    \n    // Analyze impact before proceeding\n    const impact = await this.analyzeImpact(coderOutput.diff, task);\n    if (impact.riskLevel === \"high\") {\n      task.warnings = impact.warnings;\n    }\n    \n    return coderOutput;\n  }\n  \n  async onTaskComplete(task: Task, commitSha: string): Promise<void> {\n    await this.updateKnowledgeGraph(task, commitSha);\n  }\n}\n```\n\n### Configuration\n\n```bash\n# Environment variables\nENABLE_KNOWLEDGE_GRAPH=true\nKNOWLEDGE_GRAPH_MAX_HOPS=3\nKNOWLEDGE_GRAPH_SYNC_ON_PUSH=true\n```\n\n## Acceptance Criteria\n- [ ] KnowledgeGraphService class integrating all components\n- [ ] Pre-coding context enhancement working\n- [ ] Impact analysis before diff application\n- [ ] Post-apply knowledge graph updates\n- [ ] Enhanced fixer context with entity history\n- [ ] Feature flag to enable/disable (ENABLE_KNOWLEDGE_GRAPH)\n- [ ] Integration tests with mock knowledge graph\n\n## Complexity\nM - Integration across multiple components\n\n## Dependencies\n- #230 Entity Extraction Agent\n- #231 Entity Resolution\n- #232 Temporal Validity Tracker\n- #233 Invalidation Agent\n- #234 Multi-Hop Retrieval\n- #235 Database Schema",
    "number": 236,
    "title": "feat: Integrate Knowledge Graph with AutoDev Orchestrator",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/236"
  },
  {
    "body": "## Summary\n\nCreate the database schema and migrations for the Temporal Knowledge Graph system in Neon PostgreSQL.\n\n## Background\n\nThe Knowledge Graph requires several tables to store entities, relationships, and temporal metadata. This issue covers the foundational database work.\n\n## Requirements\n\n### Tables\n\n#### 1. knowledge_entities\nStores extracted entities with temporal validity.\n\n```sql\nCREATE TABLE knowledge_entities (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  canonical_id UUID NOT NULL,\n  \n  -- Entity identification\n  entity_type VARCHAR(50) NOT NULL,  -- function, class, api, constant, type\n  name VARCHAR(255) NOT NULL,\n  file_path TEXT NOT NULL,\n  line_start INTEGER,\n  line_end INTEGER,\n  \n  -- Temporal bounds\n  valid_from TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n  valid_until TIMESTAMPTZ,  -- NULL = currently valid\n  commit_sha VARCHAR(40),\n  version INTEGER NOT NULL DEFAULT 1,\n  \n  -- Supersession chain\n  supersedes UUID REFERENCES knowledge_entities(id),\n  superseded_by UUID REFERENCES knowledge_entities(id),\n  \n  -- Full entity data\n  signature TEXT,\n  entity_data JSONB NOT NULL DEFAULT '{}',\n  \n  -- Extraction metadata\n  confidence DECIMAL(3,2),\n  extracted_at TIMESTAMPTZ DEFAULT NOW(),\n  \n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Indices\nCREATE INDEX idx_ke_canonical ON knowledge_entities(canonical_id);\nCREATE INDEX idx_ke_temporal ON knowledge_entities(valid_from, valid_until);\nCREATE INDEX idx_ke_current ON knowledge_entities(canonical_id) WHERE valid_until IS NULL;\nCREATE INDEX idx_ke_type ON knowledge_entities(entity_type);\nCREATE INDEX idx_ke_file ON knowledge_entities(file_path);\nCREATE INDEX idx_ke_name ON knowledge_entities(name);\nCREATE INDEX idx_ke_commit ON knowledge_entities(commit_sha);\n```\n\n#### 2. entity_relationships\nStores edges between entities.\n\n```sql\nCREATE TABLE entity_relationships (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  source_id UUID NOT NULL REFERENCES knowledge_entities(id) ON DELETE CASCADE,\n  target_id UUID NOT NULL REFERENCES knowledge_entities(id) ON DELETE CASCADE,\n  relationship_type VARCHAR(50) NOT NULL,  -- imports, extends, uses, etc.\n  \n  -- Temporal bounds (relationship validity)\n  valid_from TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n  valid_until TIMESTAMPTZ,\n  \n  -- Metadata\n  metadata JSONB DEFAULT '{}',\n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  \n  UNIQUE(source_id, target_id, relationship_type, valid_from)\n);\n\nCREATE INDEX idx_er_source ON entity_relationships(source_id);\nCREATE INDEX idx_er_target ON entity_relationships(target_id);\nCREATE INDEX idx_er_type ON entity_relationships(relationship_type);\nCREATE INDEX idx_er_current ON entity_relationships(source_id, target_id) WHERE valid_until IS NULL;\n```\n\n#### 3. invalidation_events\nAudit log of invalidation decisions.\n\n```sql\nCREATE TABLE invalidation_events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  entity_id UUID NOT NULL REFERENCES knowledge_entities(id),\n  reason VARCHAR(50) NOT NULL,  -- deleted, superseded, semantic_change, cascade\n  superseded_by UUID REFERENCES knowledge_entities(id),\n  commit_sha VARCHAR(40),\n  confidence DECIMAL(3,2),\n  details TEXT,\n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE INDEX idx_ie_entity ON invalidation_events(entity_id);\nCREATE INDEX idx_ie_created ON invalidation_events(created_at);\n```\n\n#### 4. knowledge_graph_sync\nTracks sync state per repository.\n\n```sql\nCREATE TABLE knowledge_graph_sync (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  repo_full_name VARCHAR(255) NOT NULL UNIQUE,\n  last_commit_sha VARCHAR(40),\n  last_sync_at TIMESTAMPTZ,\n  entity_count INTEGER DEFAULT 0,\n  status VARCHAR(50) DEFAULT 'pending',  -- pending, syncing, synced, failed\n  error_message TEXT,\n  created_at TIMESTAMPTZ DEFAULT NOW(),\n  updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n### Migration File\nCreate `src/lib/migrations/003_knowledge_graph.ts`:\n\n```typescript\nexport async function up(db: Database): Promise<void> {\n  // Create tables in order (respecting foreign keys)\n  await db.query(/* knowledge_entities */);\n  await db.query(/* entity_relationships */);\n  await db.query(/* invalidation_events */);\n  await db.query(/* knowledge_graph_sync */);\n}\n\nexport async function down(db: Database): Promise<void> {\n  await db.query(\"DROP TABLE IF EXISTS invalidation_events CASCADE\");\n  await db.query(\"DROP TABLE IF EXISTS entity_relationships CASCADE\");\n  await db.query(\"DROP TABLE IF EXISTS knowledge_graph_sync CASCADE\");\n  await db.query(\"DROP TABLE IF EXISTS knowledge_entities CASCADE\");\n}\n```\n\n## Acceptance Criteria\n- [ ] Migration file created at `src/lib/migrations/003_knowledge_graph.ts`\n- [ ] All 4 tables created with proper indices\n- [ ] Foreign key constraints working\n- [ ] Migration runs successfully on Neon\n- [ ] Rollback (down) works correctly\n- [ ] TypeScript types generated for tables\n\n## Complexity\nS - SQL schema, straightforward\n\n## Dependencies\nNone - this is foundational",
    "number": 235,
    "title": "feat: Knowledge Graph Database Schema and Migrations",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/235"
  },
  {
    "body": "## Summary\n\nImplement Multi-Hop Retrieval that traverses relationships in the Knowledge Graph to find connected entities, enabling queries like \"What files are affected if I change function X?\"\n\n## Background\n\nSingle-hop retrieval only finds directly matching entities. Multi-hop retrieval follows relationships:\n\n```\nQuery: \"What breaks if I change getUserById?\"\n                    \nSingle-hop: Returns just getUserById entity\n\nMulti-hop:  getUserById \n               ↓ used_by\n            UserService.getUser()\n               ↓ used_by\n            AuthController.login()\n               ↓ used_by\n            /api/auth/login endpoint\n\n→ Returns full impact chain with 4 entities\n```\n\n## Requirements\n\n### Relationship Types\n```typescript\ntype RelationshipType =\n  | \"imports\"      // A imports B\n  | \"exports\"      // A exports B  \n  | \"extends\"      // A extends B (class inheritance)\n  | \"implements\"   // A implements B (interface)\n  | \"uses\"         // A calls/references B\n  | \"used_by\"      // Inverse of uses\n  | \"contains\"     // File contains function\n  | \"depends_on\"   // Package dependency\n  | \"supersedes\";  // Temporal: A replaced B\n```\n\n### Query Interface\n```typescript\ninterface HopQuery {\n  startEntityId: string;\n  relationshipTypes: RelationshipType[];  // Which edges to follow\n  direction: \"outbound\" | \"inbound\" | \"both\";\n  maxHops: number;                         // Depth limit (default: 3)\n  includeInvalid: boolean;                 // Include superseded entities?\n  asOfTime?: Date;                         // Point-in-time query\n}\n\ninterface HopResult {\n  entity: TemporalEntity;\n  hopDistance: number;\n  path: {\n    relationship: RelationshipType;\n    fromEntity: string;\n    toEntity: string;\n  }[];\n}\n```\n\n### Implementation\n1. Create `src/core/knowledge-graph/multi-hop-retriever.ts`\n2. Implement BFS/DFS traversal with cycle detection\n3. Support filtered traversal (only certain relationship types)\n4. Respect temporal validity (don't traverse to invalid entities unless requested)\n5. Optimize with graph indices\n\n### Key Methods\n```typescript\nclass MultiHopRetriever {\n  // Find all entities reachable from start\n  async traverse(query: HopQuery): Promise<HopResult[]>;\n  \n  // Find impact of changing an entity\n  async findImpact(entityId: string, maxHops?: number): Promise<HopResult[]>;\n  \n  // Find all dependencies of an entity\n  async findDependencies(entityId: string, maxHops?: number): Promise<HopResult[]>;\n  \n  // Find shortest path between two entities\n  async findPath(fromId: string, toId: string): Promise<HopResult | null>;\n}\n```\n\n### Database Query (PostgreSQL recursive CTE)\n```sql\nWITH RECURSIVE entity_graph AS (\n  -- Base case: starting entity\n  SELECT id, canonical_id, entity_data, 0 as hop_distance, \n         ARRAY[id] as path\n  FROM knowledge_entities\n  WHERE id = $1 AND valid_until IS NULL\n  \n  UNION ALL\n  \n  -- Recursive case: follow relationships\n  SELECT e.id, e.canonical_id, e.entity_data, g.hop_distance + 1,\n         g.path || e.id\n  FROM knowledge_entities e\n  JOIN entity_relationships r ON e.id = r.target_id\n  JOIN entity_graph g ON r.source_id = g.id\n  WHERE g.hop_distance < $2  -- max hops\n    AND NOT e.id = ANY(g.path)  -- cycle detection\n    AND e.valid_until IS NULL\n)\nSELECT * FROM entity_graph;\n```\n\n## Acceptance Criteria\n- [ ] MultiHopRetriever class implemented\n- [ ] BFS traversal with configurable depth\n- [ ] Cycle detection to prevent infinite loops\n- [ ] Temporal filtering (current vs point-in-time)\n- [ ] findImpact() for change impact analysis\n- [ ] findDependencies() for dependency tree\n- [ ] Efficient recursive CTE query\n- [ ] Unit tests for traversal scenarios\n\n## Complexity\nM - Graph traversal + recursive SQL\n\n## Dependencies\n- #232 Temporal Validity Tracker\n- #231 Entity Resolution (for relationships)",
    "number": 234,
    "title": "feat: Implement Multi-Hop Retrieval for Knowledge Graph",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/234"
  },
  {
    "body": "## Summary\n\nImplement an Invalidation Agent that detects when new information contradicts existing knowledge and marks outdated entities as superseded.\n\n## Background\n\nWhen code changes, some facts become invalid:\n- Function signature changes → old signature invalid\n- File renamed → old path invalid  \n- Constant value changed → old value invalid\n- Function deleted → entity should be marked as ended\n\nThe Invalidation Agent automatically detects these contradictions and updates temporal validity.\n\n## Requirements\n\n### Invalidation Triggers\n1. **Direct Contradiction**: New entity has same canonicalId but different value\n2. **Deletion**: Entity existed in commit N but not in commit N+1\n3. **Semantic Change**: LLM detects breaking change in behavior\n4. **Cascade**: If A depends on B and B is invalidated, flag A for review\n\n### Invalidation Types\n```typescript\ntype InvalidationReason = \n  | \"deleted\"           // Entity no longer exists\n  | \"superseded\"        // New version exists\n  | \"signature_change\"  // Breaking API change\n  | \"semantic_change\"   // Behavior changed\n  | \"cascade\"           // Dependency was invalidated\n  | \"manual\";           // Human marked as invalid\n\ninterface InvalidationEvent {\n  entityId: string;\n  reason: InvalidationReason;\n  supersededBy?: string;\n  detectedAt: Date;\n  commitSha: string;\n  confidence: number;\n  details: string;\n}\n```\n\n### Implementation\n1. Create `src/agents/invalidation-agent.ts` extending BaseAgent\n2. Compare old vs new entity states\n3. Use LLM for semantic change detection\n4. Emit invalidation events for downstream processing\n5. Support cascade invalidation with configurable depth\n\n### Agent Flow\n```\nInput: {\n  oldEntities: TemporalEntity[],   // Current knowledge\n  newEntities: ResolvedEntity[],   // Fresh extraction\n  commitSha: string\n}\n\nOutput: {\n  invalidations: InvalidationEvent[],\n  updates: TemporalEntity[],       // Entities that need version bump\n  unchanged: string[]              // Entity IDs with no changes\n}\n```\n\n## Acceptance Criteria\n- [ ] InvalidationAgent class implemented\n- [ ] Detects deletions (entity missing from new extraction)\n- [ ] Detects supersession (same entity, different content)\n- [ ] LLM-based semantic change detection\n- [ ] Cascade invalidation for dependencies\n- [ ] Confidence scoring for invalidation decisions\n- [ ] Integration with TemporalTracker\n\n## Complexity\nM - Agent + comparison logic + LLM calls\n\n## Dependencies\n- #230 Entity Extraction Agent\n- #231 Entity Resolution\n- #232 Temporal Validity Tracker",
    "number": 233,
    "title": "feat: Implement Invalidation Agent",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/233"
  },
  {
    "body": "## Summary\n\nImplement a Temporal Validity Tracker that manages time-bounded facts in the Knowledge Graph, tracking when entities were valid and detecting state changes.\n\n## Background\n\nCode changes over time. A function that existed yesterday might be deleted today. The Temporal Validity Tracker:\n1. Assigns `valid_from` timestamps when entities are first seen\n2. Assigns `valid_until` timestamps when entities are superseded or deleted\n3. Enables temporal queries (\"What did this function look like in commit X?\")\n\n## Requirements\n\n### Temporal Schema\n```typescript\ninterface TemporalEntity {\n  id: string;\n  canonicalId: string;\n  \n  // Temporal bounds\n  validFrom: Date;           // When this version became active\n  validUntil: Date | null;   // null = still current\n  \n  // Version tracking\n  commitSha: string;         // Git commit where this was observed\n  version: number;           // Incrementing version number\n  \n  // Supersession chain\n  supersedes?: string;       // ID of previous version\n  supersededBy?: string;     // ID of next version (set when invalidated)\n  \n  // The actual entity data\n  entity: ResolvedEntity;\n}\n```\n\n### Database Schema (Neon PostgreSQL)\n```sql\nCREATE TABLE knowledge_entities (\n  id UUID PRIMARY KEY,\n  canonical_id UUID NOT NULL,\n  valid_from TIMESTAMPTZ NOT NULL,\n  valid_until TIMESTAMPTZ,\n  commit_sha VARCHAR(40),\n  version INTEGER NOT NULL,\n  supersedes UUID REFERENCES knowledge_entities(id),\n  entity_data JSONB NOT NULL,\n  created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE INDEX idx_entities_canonical ON knowledge_entities(canonical_id);\nCREATE INDEX idx_entities_temporal ON knowledge_entities(valid_from, valid_until);\nCREATE INDEX idx_entities_current ON knowledge_entities(canonical_id) WHERE valid_until IS NULL;\n```\n\n### Implementation\n1. Create `src/core/knowledge-graph/temporal-tracker.ts`\n2. Create migration for knowledge_entities table\n3. Implement version comparison logic\n4. Support point-in-time queries\n\n### Key Methods\n```typescript\nclass TemporalTracker {\n  // Record new entity version\n  async recordVersion(entity: ResolvedEntity, commitSha: string): Promise<TemporalEntity>;\n  \n  // Get current version of entity\n  async getCurrent(canonicalId: string): Promise<TemporalEntity | null>;\n  \n  // Get entity at specific point in time\n  async getAtTime(canonicalId: string, timestamp: Date): Promise<TemporalEntity | null>;\n  \n  // Get full history of entity\n  async getHistory(canonicalId: string): Promise<TemporalEntity[]>;\n  \n  // Invalidate entity (mark as superseded)\n  async invalidate(entityId: string, supersededBy?: string): Promise<void>;\n}\n```\n\n## Acceptance Criteria\n- [ ] Database migration for knowledge_entities table\n- [ ] TemporalTracker class with CRUD operations\n- [ ] Point-in-time query support\n- [ ] Version chain tracking (supersedes/supersededBy)\n- [ ] Index optimization for temporal queries\n- [ ] Unit tests for temporal operations\n\n## Complexity\nM - Database schema + temporal logic\n\n## Dependencies\n- #231 Entity Resolution",
    "number": 232,
    "title": "feat: Implement Temporal Validity Tracker",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/232"
  },
  {
    "body": "## Summary\n\nImplement Entity Resolution that deduplicates extracted entities and links them to existing entities in the Knowledge Graph.\n\n## Background\n\nWhen extracting entities from multiple files or across time, the same logical entity may be extracted multiple times. Entity Resolution:\n1. Identifies when two extractions refer to the same entity\n2. Merges metadata from multiple sources\n3. Creates relationships between related entities\n\n## Requirements\n\n### Resolution Strategies\n- **Exact Match**: Same name + same file path\n- **Signature Match**: Same function signature, different location (moved/renamed file)\n- **Fuzzy Match**: Similar names with high cosine similarity (refactored names)\n- **Relationship Inference**: If A imports B, create dependency edge\n\n### Output\n```typescript\ninterface ResolvedEntity extends ExtractedEntity {\n  canonicalId: string;        // Stable ID across extractions\n  aliases: string[];          // Previous names/locations\n  relationships: {\n    type: \"imports\" | \"extends\" | \"implements\" | \"uses\" | \"supersedes\";\n    targetId: string;\n  }[];\n  mergedFrom: string[];       // IDs of extracted entities that were merged\n}\n```\n\n### Implementation\n1. Create `src/core/knowledge-graph/entity-resolver.ts`\n2. Use embedding similarity for fuzzy matching\n3. Track entity lineage (what was merged into what)\n4. Handle rename detection (file moved, function renamed)\n\n## Acceptance Criteria\n- [ ] EntityResolver class implemented\n- [ ] Exact and signature matching working\n- [ ] Fuzzy matching with configurable threshold\n- [ ] Relationship inference from imports/extends\n- [ ] Merge history tracked\n- [ ] Unit tests for resolution scenarios\n\n## Complexity\nS - Algorithmic but bounded scope\n\n## Dependencies\n- #230 Entity Extraction Agent",
    "number": 231,
    "title": "feat: Implement Entity Resolution and Deduplication",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/231"
  },
  {
    "body": "## Summary\n\nImplement an Entity Extraction Agent that extracts structured entities from codebase files for the Temporal Knowledge Graph system.\n\n## Background\n\nTraditional RAG has limitations with stale information and no time awareness. A Temporal Knowledge Graph tracks entities with temporal validity, enabling queries like \"What was the API signature last week?\" or \"When did this function change?\"\n\n## Requirements\n\n### Entity Types to Extract\n- **Functions**: name, signature, file, line range, dependencies\n- **Classes**: name, methods, properties, inheritance\n- **APIs**: endpoints, parameters, return types\n- **Constants/Config**: name, value, usage locations\n- **Types/Interfaces**: name, properties, used by\n\n### Output Schema\n```typescript\ninterface ExtractedEntity {\n  id: string;\n  type: \"function\" | \"class\" | \"api\" | \"constant\" | \"type\";\n  name: string;\n  filePath: string;\n  lineStart: number;\n  lineEnd: number;\n  signature?: string;\n  dependencies: string[];  // references to other entity IDs\n  metadata: Record<string, unknown>;\n  extractedAt: Date;\n  confidence: number;\n}\n```\n\n### Implementation\n1. Create `src/agents/entity-extractor.ts` extending BaseAgent\n2. Use LLM to parse code and extract structured entities\n3. Support TypeScript, JavaScript initially (extensible to other languages)\n4. Include confidence scores for extraction quality\n\n## Acceptance Criteria\n- [ ] EntityExtractorAgent class implemented\n- [ ] Extracts functions, classes, and types from TS/JS files\n- [ ] Returns structured entities with confidence scores\n- [ ] Unit tests for extraction accuracy\n- [ ] Integrates with existing agent pattern (BaseAgent)\n\n## Complexity\nS - Single agent, well-defined input/output\n\n## References\n- OpenAI Cookbook: Temporal Agents with Knowledge Graphs\n- Existing agents in `src/agents/`",
    "number": 230,
    "title": "feat: Implement Entity Extraction Agent for Knowledge Graph",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/230"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nCreate end-to-end tests for MCP server.\n\n## Implementation\nCreate `tests/mcp-server.test.ts`:\n\nTest cases:\n1. Server starts and responds to handshake\n2. tools/list returns all 4 tools\n3. autodev.analyze returns valid analysis\n4. autodev.execute starts task (mocked)\n5. autodev.status returns task info\n6. autodev.memory returns domain data\n7. Invalid tool name returns error\n8. Invalid arguments return error\n\nUse mocked dependencies for deterministic tests.\n\n## Definition of Done\n- [ ] Create test file\n- [ ] Test server startup\n- [ ] Test each tool\n- [ ] Test error handling\n- [ ] All tests pass\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 229,
    "title": "[#135 Part 8/8] Create MCP server end-to-end tests",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/229"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nCreate configuration examples for Cursor and VS Code Continue.\n\n## Implementation\nCreate `docs/mcp-setup.md` with:\n\n1. Cursor configuration (~/.cursor/mcp.json)\n2. VS Code Continue configuration (.continue/config.json)\n3. Environment variables needed\n4. How to test the connection\n5. Usage examples in chat\n\nAlso create example config files:\n- `examples/cursor-mcp.json`\n- `examples/continue-config.json`\n\n## Definition of Done\n- [ ] Create docs/mcp-setup.md\n- [ ] Cursor config example\n- [ ] Continue config example\n- [ ] List required env vars\n- [ ] Usage examples\n- [ ] Troubleshooting section\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 228,
    "title": "[#135 Part 7/8] Create editor configuration documentation",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/228"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nRegister all tools and create main tool handler router.\n\n## Implementation\nUpdate `src/mcp/server.ts`:\n\n- Import all tool definitions\n- Register tools in tools/list handler\n- Create router for tools/call handler\n- Dispatch to appropriate handler based on tool name\n- Handle unknown tools gracefully\n\n```typescript\nserver.setRequestHandler(\"tools/list\", async () => ({\n  tools: [analyzeTool, executeTool, statusTool, memoryTool]\n}));\n\nserver.setRequestHandler(\"tools/call\", async (request) => {\n  switch (request.params.name) {\n    case \"autodev.analyze\": return handleAnalyze(args);\n    // ...\n  }\n});\n```\n\n## Definition of Done\n- [ ] Register all 4 tools\n- [ ] Implement tool router\n- [ ] Handle unknown tools\n- [ ] Test tools/list response\n- [ ] Test each tool call\n\n## Complexity: XS\n## Estimate: 30 minutes\n",
    "number": 227,
    "title": "[#135 Part 6/8] Register tools and create handler router",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/227"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nImplement the autodev.memory tool for querying domain memory.\n\n## Implementation\nCreate `src/mcp/tools/memory.ts`:\n\n- Tool definition with input schema (repo, query)\n- Query types: \"config\", \"recent_tasks\", \"patterns\", \"decisions\"\n- Handler that fetches from learning memory store\n- Return appropriate data based on query type\n\n## Definition of Done\n- [ ] Create tool definition\n- [ ] Implement handler\n- [ ] Support all query types\n- [ ] Return structured data\n- [ ] Handle repo not found\n- [ ] Test each query type\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 226,
    "title": "[#135 Part 5/8] Implement autodev.memory tool",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/226"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nImplement the autodev.status tool for checking task progress.\n\n## Implementation\nCreate `src/mcp/tools/status.ts`:\n\n- Tool definition with input schema (taskId)\n- Handler that fetches task from database\n- Return status, phase, attempts, progress\n- Include prUrl if PR created\n- Include lastError if failed\n\n## Definition of Done\n- [ ] Create tool definition\n- [ ] Implement handler\n- [ ] Fetch task from DB\n- [ ] Include progress entries\n- [ ] Handle task not found\n- [ ] Test with running task\n\n## Complexity: XS\n## Estimate: 30 minutes\n",
    "number": 225,
    "title": "[#135 Part 4/8] Implement autodev.status tool",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/225"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nImplement the autodev.execute tool for running the full pipeline.\n\n## Implementation\nCreate `src/mcp/tools/execute.ts`:\n\n- Tool definition with input schema (repo, issueNumber, dryRun)\n- Handler that creates task and triggers processing\n- If dryRun: run until CODING_DONE, return diff\n- If not dryRun: start async, return taskId immediately\n- Return status and prUrl when complete\n\n## Definition of Done\n- [ ] Create tool definition\n- [ ] Implement handler\n- [ ] Support dryRun mode\n- [ ] Async execution for full pipeline\n- [ ] Return appropriate results\n- [ ] Handle errors\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 224,
    "title": "[#135 Part 3/8] Implement autodev.execute tool",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/224"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nImplement the autodev.analyze tool for previewing issue analysis.\n\n## Implementation\nCreate `src/mcp/tools/analyze.ts`:\n\n- Tool definition with input schema (repo, issueNumber)\n- Handler that fetches issue from GitHub\n- Runs analysis (reuse PlannerAgent logic)\n- Returns complexity, targetFiles, plan, confidence\n- Recommendation: \"execute\" | \"breakdown\" | \"manual\"\n\n## Definition of Done\n- [ ] Create tool definition\n- [ ] Implement handler\n- [ ] Fetch GitHub issue\n- [ ] Return analysis results\n- [ ] Handle errors gracefully\n- [ ] Test with sample issue\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 223,
    "title": "[#135 Part 2/8] Implement autodev.analyze tool",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/223"
  },
  {
    "body": "## Parent Issue\nPart of #135 - MCP Server - Editor Integration\n\n## Goal\nSet up MCP SDK and create basic server structure.\n\n## Implementation\n1. Add dependency: `@modelcontextprotocol/sdk`\n2. Create `src/mcp/server.ts` with basic server setup\n3. Create `src/mcp/types.ts` for MCP-specific types\n\nBasic structure:\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio\";\n\nconst server = new Server({\n  name: \"autodev\",\n  version: \"1.0.0\"\n}, {\n  capabilities: { tools: {} }\n});\n\n// Start server\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n## Definition of Done\n- [ ] Add @modelcontextprotocol/sdk to package.json\n- [ ] Create src/mcp/server.ts\n- [ ] Create src/mcp/types.ts\n- [ ] Server starts without errors\n- [ ] Responds to basic protocol handshake\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 222,
    "title": "[#135 Part 1/8] Set up MCP SDK and basic server structure",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/222"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nCreate end-to-end tests for the agentic loop with complex test cases.\n\n## Implementation\nCreate `tests/agentic-loop.test.ts`:\n\nTest cases:\n1. Simple fix (no replan needed)\n2. Replan after reflection identifies plan issue\n3. Multiple iterations until success\n4. Abort when confidence too low\n5. Max iterations exceeded\n\nMock agents for deterministic testing.\n\n## Definition of Done\n- [ ] Create test file\n- [ ] Test simple fix path\n- [ ] Test replan path\n- [ ] Test abort conditions\n- [ ] Test max iterations\n- [ ] All tests pass\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 221,
    "title": "[#193 Part 10/10] Create agentic loop end-to-end tests",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/221"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nAdd metrics tracking for agentic loop performance.\n\n## Implementation\nModify `src/integrations/db.ts` and create metrics helpers:\n\nTrack per task:\n- Total iterations\n- Replan count\n- Final confidence score\n- Time per iteration\n- Success after N iterations distribution\n\nAdd to task_events:\n- `REFLECTION_COMPLETE` event type\n- `REPLAN_TRIGGERED` event type\n- Store reflection output in event data\n\n## Definition of Done\n- [ ] Add new event types\n- [ ] Track iterations and replans in task\n- [ ] Store reflection output\n- [ ] Add /api/analytics/agentic endpoint (optional)\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 220,
    "title": "[#193 Part 9/10] Add agentic loop metrics and tracking",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/220"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nIntegrate the agentic loop into the main orchestrator.\n\n## Implementation\nModify `src/core/orchestrator.ts`:\n\n- Add `useAgenticLoop` config option\n- When enabled, use AgenticLoopController instead of simple fix loop\n- Pass iteration memory between calls\n- Track agentic metrics in task events\n\n```typescript\nif (config.useAgenticLoop && task.status === \"TESTS_FAILED\") {\n  const loopController = new AgenticLoopController();\n  const result = await loopController.run(task, loopConfig);\n  // Handle result\n}\n```\n\n## Definition of Done\n- [ ] Add useAgenticLoop config option\n- [ ] Integrate AgenticLoopController\n- [ ] Track metrics in task events\n- [ ] Fallback to simple loop if disabled\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 219,
    "title": "[#193 Part 8/10] Integrate agentic loop into orchestrator",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/219"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nAdd new task states for agentic loop phases.\n\n## Implementation\nModify `src/core/types.ts` and `src/core/state-machine.ts`:\n\nNew states:\n- `REFLECTING` - analyzing failure\n- `REPLANNING` - creating new plan based on feedback\n\nTransitions:\n- `TESTS_FAILED` → `REFLECTING`\n- `REFLECTING` → `REPLANNING` (if rootCause is plan)\n- `REFLECTING` → `FIXING` (if rootCause is code)\n- `REPLANNING` → `CODING`\n\n## Definition of Done\n- [ ] Add REFLECTING and REPLANNING states\n- [ ] Update state machine transitions\n- [ ] Update getNextAction() for new states\n- [ ] Update Task type if needed\n\n## Complexity: XS\n## Estimate: 30 minutes\n",
    "number": 218,
    "title": "[#193 Part 7/10] Add REFLECTING and REPLANNING task states",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/218"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nModify FixerAgent to use reflection feedback for more targeted fixes.\n\n## Implementation\nModify `src/agents/fixer.ts`:\n\n- Add optional `reflectionFeedback` to FixerInput\n- Add `rootCause` from reflection to guide fix\n- Update prompt to use reflection insights\n- Prioritize fix based on diagnosis\n\nExample:\n```typescript\ninterface FixerInput {\n  // ... existing fields\n  reflectionFeedback?: string;\n  rootCause?: \"plan\" | \"code\" | \"test\" | \"environment\";\n}\n```\n\n## Definition of Done\n- [ ] Add reflection fields to FixerInput\n- [ ] Update fixer.md prompt\n- [ ] Use rootCause to guide fix strategy\n- [ ] Test with reflection output\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 217,
    "title": "[#193 Part 6/10] Modify FixerAgent to use reflection feedback",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/217"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nModify PlannerAgent to accept feedback from previous iterations.\n\n## Implementation\nModify `src/agents/planner.ts`:\n\n- Add optional `previousFeedback` to PlannerInput\n- Add optional `failedApproaches` to avoid repeating\n- Update prompt to include feedback context\n- Adjust plan based on what didn't work\n\nExample:\n```typescript\ninterface PlannerInput {\n  // ... existing fields\n  previousFeedback?: string;\n  failedApproaches?: string[];\n}\n```\n\nPrompt addition:\n```\nPrevious attempt failed because: {feedback}\nAvoid these approaches: {failedApproaches}\n```\n\n## Definition of Done\n- [ ] Add feedback fields to PlannerInput\n- [ ] Update planner.md prompt\n- [ ] Include feedback in planning context\n- [ ] Test replanning with feedback\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 216,
    "title": "[#193 Part 5/10] Modify PlannerAgent to accept iteration feedback",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/216"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nCreate the main agentic loop controller that orchestrates the plan-code-test-reflect cycle.\n\n## Implementation\nCreate `src/core/agentic/loop-controller.ts`:\n\n- `AgenticLoopController` class:\n  - `run(task: Task, config: LoopConfig): Promise<LoopResult>`\n  - Orchestrate: Plan → Code → Test → Reflect → (Replan|Fix)\n  - Track iterations and replans\n  - Check confidence threshold\n  - Return final result with metrics\n\nFlow:\n1. Plan (with previous feedback if any)\n2. Code (generate diff)\n3. Test (run tests)\n4. If pass → Review → Success\n5. If fail → Reflect → Replan or Fix → Loop\n\n## Definition of Done\n- [ ] Create `src/core/agentic/loop-controller.ts`\n- [ ] Implement full loop logic\n- [ ] Respect maxIterations and maxReplans\n- [ ] Check confidence threshold\n- [ ] Return detailed LoopResult\n\n## Complexity: XS\n## Estimate: 1.5 hours\n",
    "number": 215,
    "title": "[#193 Part 4/10] Create agentic loop controller",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/215"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nCreate iteration memory to track what was tried and prevent repeating mistakes.\n\n## Implementation\nCreate `src/core/agentic/iteration-memory.ts`:\n\n- `IterationMemory` class:\n  - `addAttempt(record: AttemptRecord)` - record an attempt\n  - `getAttempts(): AttemptRecord[]` - get all attempts\n  - `getFailedApproaches(): string[]` - list failed strategies\n  - `hasTriedApproach(approach: string): boolean` - check if tried\n  - `getSummary(): string` - human-readable summary for prompts\n- In-memory storage per task\n- Include in agent prompts to avoid repeating mistakes\n\n## Definition of Done\n- [ ] Create `src/core/agentic/iteration-memory.ts`\n- [ ] Track all attempts with details\n- [ ] Detect repeated approaches\n- [ ] Generate summary for prompts\n- [ ] Unit tests\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 214,
    "title": "[#193 Part 3/10] Create iteration memory for tracking attempts",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/214"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nCreate the ReflectionAgent that analyzes test failures and determines next action.\n\n## Implementation\nCreate `src/agents/reflection.ts`:\n\n- Extend BaseAgent with ReflectionInput/ReflectionOutput\n- Analyze test output to diagnose failure\n- Determine root cause (plan vs code vs test vs environment)\n- Recommend next action (replan, fix, or abort)\n- Calculate confidence score based on error clarity\n- Provide specific feedback for next iteration\n\nPrompt should:\n- Analyze the gap between expected and actual behavior\n- Identify if the plan was flawed or implementation was wrong\n- Give actionable feedback\n\n## Definition of Done\n- [ ] Create `src/agents/reflection.ts`\n- [ ] Create `prompts/reflection.md`\n- [ ] Diagnose failures accurately\n- [ ] Output confidence score 0-1\n- [ ] Basic tests with sample failures\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 213,
    "title": "[#193 Part 2/10] Create ReflectionAgent for failure analysis",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/213"
  },
  {
    "body": "## Parent Issue\nPart of #193 - Agentic Loop with Self-Correction\n\n## Goal\nDefine TypeScript types for the reflection and agentic loop system.\n\n## Implementation\nCreate `src/core/agentic/types.ts`:\n\n```typescript\nexport interface ReflectionInput {\n  originalIssue: string;\n  plan: string[];\n  diff: string;\n  testOutput: string;\n  attemptNumber: number;\n  previousAttempts: AttemptRecord[];\n}\n\nexport interface ReflectionOutput {\n  diagnosis: string;\n  rootCause: \"plan\" | \"code\" | \"test\" | \"environment\";\n  recommendation: \"replan\" | \"fix\" | \"abort\";\n  feedback: string;\n  confidence: number;\n}\n\nexport interface AttemptRecord {\n  iteration: number;\n  action: \"plan\" | \"code\" | \"fix\";\n  result: \"success\" | \"failure\";\n  error?: string;\n  timestamp: Date;\n}\n\nexport interface LoopConfig {\n  maxIterations: number;\n  maxReplans: number;\n  confidenceThreshold: number;\n}\n\nexport interface LoopResult {\n  success: boolean;\n  iterations: number;\n  replans: number;\n  finalDiff?: string;\n  reason?: string;\n}\n```\n\n## Definition of Done\n- [ ] Create `src/core/agentic/types.ts`\n- [ ] Export all types\n- [ ] Add Zod schemas for validation\n- [ ] Types pass typecheck\n\n## Complexity: XS\n## Estimate: 30 minutes\n",
    "number": 212,
    "title": "[#193 Part 1/10] Create agentic loop types and interfaces",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/212"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nInitialize RAG index when processing tasks and add API endpoint for manual re-indexing.\n\n## Implementation\n\n1. Modify `src/core/orchestrator.ts`:\n   - Initialize RAG on first task if not already initialized\n   - Clone repo if needed, then index\n\n2. Add API endpoint in `src/router.ts`:\n   - `POST /api/rag/index` - trigger re-indexing\n   - `GET /api/rag/stats` - get index statistics\n   - `POST /api/rag/search` - manual search endpoint\n\n## Definition of Done\n- [ ] Auto-initialize RAG in orchestrator\n- [ ] POST /api/rag/index endpoint\n- [ ] GET /api/rag/stats endpoint\n- [ ] POST /api/rag/search endpoint\n- [ ] Test endpoints manually\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 211,
    "title": "[#196 Part 12/12] Add RAG initialization and API endpoints",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/211"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nAdd incremental update mechanism to re-index only changed files.\n\n## Implementation\nAdd to `src/services/rag/codebase-index.ts`:\n\n- `updateFile(filePath: string)` - re-index single changed file\n- `removeFile(filePath: string)` - remove deleted file from index\n- Track file hashes to detect changes\n- `syncWithFilesystem()` - detect and sync all changes\n\nStorage:\n- Store file hash -> chunk IDs mapping\n- On file change: remove old chunks, add new chunks\n\n## Definition of Done\n- [ ] Add updateFile() method\n- [ ] Add removeFile() method\n- [ ] Track file content hashes\n- [ ] Detect changed files efficiently\n- [ ] Test incremental update\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 210,
    "title": "[#196 Part 11/12] Add incremental index update mechanism",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/210"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nIntegrate RAG search into FixerAgent to find related code for context when fixing errors.\n\n## Implementation\nModify `src/agents/fixer.ts`:\n\n- When error mentions undefined symbol, search for its definition\n- Find similar error fixes in codebase history\n- Include related code in fixer context\n\nExample usage:\n```typescript\n// Find definition of undefined symbol\nconst symbolDef = await ragService.search(`function ${undefinedSymbol}`);\n// Include in fixer prompt\n```\n\n## Definition of Done\n- [ ] Integrate RAG search in FixerAgent\n- [ ] Find symbol definitions for undefined errors\n- [ ] Include related code in context\n- [ ] Graceful fallback if RAG not available\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 209,
    "title": "[#196 Part 10/12] Integrate RAG search into FixerAgent",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/209"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nIntegrate RAG search into CoderAgent to find code examples and patterns.\n\n## Implementation\nModify `src/agents/coder.ts`:\n\n- Search for similar code patterns before generating\n- Include relevant examples in prompt context\n- Find definitions of imported symbols\n- Search for existing tests as reference\n\nExample usage:\n```typescript\n// Find similar implementations\nconst examples = await ragService.search(\"user authentication middleware\");\n// Include in coder prompt as reference patterns\n```\n\n## Definition of Done\n- [ ] Integrate RAG search in CoderAgent\n- [ ] Find similar code patterns\n- [ ] Include examples in prompt context\n- [ ] Graceful fallback if RAG not available\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 208,
    "title": "[#196 Part 9/12] Integrate RAG search into CoderAgent",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/208"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nIntegrate RAG search into PlannerAgent to automatically find relevant files.\n\n## Implementation\nModify `src/agents/planner.ts`:\n\n- Before planning, search RAG for relevant code based on issue description\n- Add found files to `targetFiles` automatically\n- Include relevant code snippets in context for better planning\n- Fallback to manual file specification if RAG not initialized\n\nExample:\n```typescript\n// In PlannerAgent.run()\nif (ragService.isInitialized()) {\n  const relevantCode = await ragService.search(issue.body);\n  const suggestedFiles = relevantCode.map(r => r.chunk.filePath);\n  // Include in planning context\n}\n```\n\n## Definition of Done\n- [ ] Integrate RAG search in PlannerAgent\n- [ ] Auto-suggest targetFiles from search results\n- [ ] Include code snippets in planning context\n- [ ] Graceful fallback if RAG not available\n- [ ] Test with real issue\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 207,
    "title": "[#196 Part 8/12] Integrate RAG search into PlannerAgent",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/207"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate a singleton RAG service that can be used across the application.\n\n## Implementation\nCreate `src/services/rag/index.ts`:\n\n- Export singleton `ragService` with:\n  - `initialize(repoPath: string)` - init and index repo\n  - `search(query: string)` - search interface\n  - `isInitialized(): boolean` - check if ready\n  - `getIndex(): CodebaseIndex` - access index directly\n  - `getSearch(): CodebaseSearch` - access search directly\n- Lazy initialization\n- Thread-safe (for Bun)\n\n## Definition of Done\n- [ ] Create singleton service in `src/services/rag/index.ts`\n- [ ] Export all types\n- [ ] Lazy initialization\n- [ ] Easy access to search and index\n\n## Complexity: XS\n## Estimate: 30 minutes\n",
    "number": 206,
    "title": "[#196 Part 7/12] Create RAG service singleton and exports",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/206"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate the search interface that queries the vector store and returns relevant code.\n\n## Implementation\nCreate `src/services/rag/search.ts`:\n\n- `CodebaseSearch` class with:\n  - `search(query: string, options?: SearchOptions): Promise<SearchResult[]>`\n  - `findSimilarCode(code: string): Promise<SearchResult[]>`\n  - `findBySymbol(symbolName: string): Promise<SearchResult[]>`\n- Embed query, search vector store, format results\n- Add context (surrounding lines) to results\n- Filter by file type, exclude paths\n\n## Definition of Done\n- [ ] Create `src/services/rag/search.ts`\n- [ ] Text query search\n- [ ] Code similarity search\n- [ ] Symbol lookup\n- [ ] Add surrounding context to results\n- [ ] Support search options (limit, minScore, filters)\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 205,
    "title": "[#196 Part 6/12] Create CodebaseSearch query interface",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/205"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate the main CodebaseIndex class that orchestrates chunking, embedding, and storage.\n\n## Implementation\nCreate `src/services/rag/codebase-index.ts`:\n\n- `CodebaseIndex` class with:\n  - `indexFile(filePath: string)` - index single file\n  - `indexDirectory(dirPath: string)` - index directory recursively\n  - `getStats(): IndexStats` - return index statistics\n  - `clear()` - clear all indexed data\n- Coordinate chunker, embedder, and vector store\n- Track indexed files to avoid re-indexing\n- Skip files in .gitignore and node_modules\n\n## Definition of Done\n- [ ] Create `src/services/rag/codebase-index.ts`\n- [ ] Index single file and directory\n- [ ] Skip ignored files (node_modules, .git, etc.)\n- [ ] Track which files are indexed\n- [ ] Return index statistics\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 204,
    "title": "[#196 Part 5/12] Create CodebaseIndex orchestrator class",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/204"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate an in-memory vector store using hnswlib-node for storing and searching embeddings.\n\n## Implementation\nCreate `src/services/rag/vector-store.ts`:\n\n- `VectorStore` class with:\n  - `add(id: string, embedding: number[], metadata: object)` - add vector\n  - `search(embedding: number[], k: number): SearchResult[]` - find k nearest\n  - `delete(id: string)` - remove vector\n  - `save(path: string)` - persist to disk\n  - `load(path: string)` - load from disk\n- Use hnswlib-node for efficient similarity search\n- Store metadata in parallel Map\n\n## Dependencies\n- Add `hnswlib-node` to package.json\n\n## Definition of Done\n- [ ] Create `src/services/rag/vector-store.ts`\n- [ ] Add, search, delete operations\n- [ ] Save/load to disk for persistence\n- [ ] Unit tests for all operations\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 203,
    "title": "[#196 Part 4/12] Create in-memory vector store with hnswlib",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/203"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate an embedding service that generates vector embeddings for code chunks using OpenAI's text-embedding-3-small.\n\n## Implementation\nCreate `src/services/rag/embedder.ts`:\n\n- `generateEmbedding(text: string): Promise<number[]>` - single text embedding\n- `generateEmbeddings(texts: string[]): Promise<number[][]>` - batch embeddings\n- Use OpenAI API with `text-embedding-3-small` model\n- Handle rate limits with retry logic\n- Batch requests for efficiency (max 2048 tokens per request)\n\n## Environment\n- Uses existing `OPENAI_API_KEY` from env\n\n## Definition of Done\n- [ ] Create `src/services/rag/embedder.ts`\n- [ ] Single and batch embedding functions\n- [ ] Retry logic for rate limits\n- [ ] Token counting to stay within limits\n- [ ] Unit test with mocked OpenAI\n\n## Complexity: XS\n## Estimate: 45 minutes\n",
    "number": 202,
    "title": "[#196 Part 3/12] Create embedding service with OpenAI",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/202"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nCreate a simple regex-based chunker for TypeScript/JavaScript files that splits code into semantic units.\n\n## Implementation\nCreate `src/services/rag/chunker.ts`:\n\n- Regex patterns for functions, classes, interfaces, types\n- `chunkTypeScript(content, filePath)` - main chunker function\n- `extractImports(content)` - extract import statements\n- `extractExports(content)` - extract export statements\n- `generateChunkId(filePath, startLine)` - unique chunk IDs\n\n## Definition of Done\n- [ ] Create `src/services/rag/chunker.ts`\n- [ ] Chunk functions, classes, interfaces, types\n- [ ] Extract imports/exports\n- [ ] Generate unique chunk IDs\n- [ ] Handle arrow functions\n- [ ] Basic tests in `tests/rag-chunker.test.ts`\n\n## Complexity: XS\n## Estimate: 1 hour\n",
    "number": 201,
    "title": "[#196 Part 2/12] Create TypeScript/JavaScript chunker using regex",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/201"
  },
  {
    "body": "## Parent Issue\nPart of #196 - RAG-Based Codebase Indexing\n\n## Goal\nDefine TypeScript types for code chunking system.\n\n## Implementation\nCreate `src/services/rag/types.ts`:\n\n```typescript\nexport interface CodeChunk {\n  id: string;\n  filePath: string;\n  startLine: number;\n  endLine: number;\n  content: string;\n  type: \"function\" | \"class\" | \"interface\" | \"type\" | \"module\" | \"block\";\n  symbols: string[];    // Function/class names in chunk\n  imports: string[];    // What this chunk imports\n  exports: string[];    // What this chunk exports\n  language: string;     // typescript, javascript, etc.\n  hash: string;         // Content hash for change detection\n}\n\nexport interface SearchResult {\n  chunk: CodeChunk;\n  score: number;\n  context: string;      // Surrounding code\n}\n\nexport interface SearchOptions {\n  limit?: number;\n  minScore?: number;\n  fileTypes?: string[];\n  excludePaths?: string[];\n}\n\nexport interface IndexStats {\n  totalFiles: number;\n  totalChunks: number;\n  lastIndexed: Date;\n  languages: Record<string, number>;\n}\n```\n\n## Definition of Done\n- [ ] Create `src/services/rag/types.ts`\n- [ ] Export types from `src/services/rag/index.ts`\n- [ ] Types pass typecheck\n\n## Complexity: XS\n## Estimate: 30 minutes",
    "number": 200,
    "title": "[#196 Part 1/12] Create CodeChunk types and interfaces",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/200"
  },
  {
    "body": "## Goal\nIndex the entire codebase for semantic search, enabling MultiplAI to find relevant code without knowing exact file paths.\n\n## Why This Matters\n- Currently: Must guess file paths or grep for keywords\n- With RAG: \"Find the authentication middleware\" → exact file + line\n- Better context = fewer errors, better code generation\n\n## Implementation\n\n### Indexing Pipeline\n```\nCodebase\n    ↓\nChunker (split files into semantic units)\n    ↓\nEmbedder (generate vectors)\n    ↓\nVector Store (store with metadata)\n    ↓\nQuery Interface\n```\n\n### Chunking Strategy\n```typescript\ninterface CodeChunk {\n  id: string;\n  filePath: string;\n  startLine: number;\n  endLine: number;\n  content: string;\n  type: 'function' | 'class' | 'interface' | 'module' | 'block';\n  symbols: string[];  // Function/class names in chunk\n  imports: string[];  // What this chunk imports\n  exports: string[];  // What this chunk exports\n}\n\n// Chunk by semantic boundaries, not line count\nfunction chunkFile(content: string, language: string): CodeChunk[] {\n  // Use tree-sitter or regex to find:\n  // - Function definitions\n  // - Class definitions\n  // - Interface/type definitions\n  // - Top-level blocks\n}\n```\n\n### Embedding Options\n1. **OpenAI text-embedding-3-small** - Good quality, $0.02/1M tokens\n2. **Voyage Code** - Optimized for code\n3. **Local (all-MiniLM)** - Free but lower quality\n\n### Vector Store Options\n1. **Pinecone** - Managed, scales well\n2. **Qdrant** - Self-hosted, good performance\n3. **In-memory (hnswlib)** - Simple, for small codebases\n\n### Query Interface\n```typescript\n// src/services/codebase-search.ts\nclass CodebaseSearch {\n  async search(query: string, options?: SearchOptions): Promise<SearchResult[]> {\n    // 1. Embed query\n    // 2. Vector similarity search\n    // 3. Rerank results\n    // 4. Return with context\n  }\n  \n  async findSimilarCode(code: string): Promise<SearchResult[]> {\n    // Find code similar to a snippet\n  }\n  \n  async findBySymbol(symbolName: string): Promise<SearchResult[]> {\n    // Find where a function/class is defined\n  }\n}\n\ninterface SearchResult {\n  chunk: CodeChunk;\n  score: number;\n  context: string;  // Surrounding code for context\n}\n```\n\n### Integration Points\n\n1. **PlannerAgent** - Find relevant files automatically\n```typescript\nconst relevantCode = await search.search(issueDescription);\nconst targetFiles = [...new Set(relevantCode.map(r => r.chunk.filePath))];\n```\n\n2. **CoderAgent** - Find examples to follow\n```typescript\nconst examples = await search.findSimilarCode(\"function that validates user input\");\n// Include in prompt as reference\n```\n\n3. **FixerAgent** - Find related code for context\n```typescript\nconst related = await search.findBySymbol(undefinedSymbol);\n// Include definition in context\n```\n\n### Incremental Updates\n- Watch for file changes (git hooks or filesystem)\n- Re-index only changed files\n- Background job for full re-index\n\n## Definition of Done\n- [ ] Chunker for TypeScript/JavaScript\n- [ ] Embedding generation\n- [ ] Vector store integration\n- [ ] Search interface\n- [ ] Integration with Planner agent\n- [ ] Incremental update mechanism\n- [ ] Works for codebase up to 10k files\n\n## Complexity: L\n## Estimate: 4-5 days",
    "number": 196,
    "title": "[Core] RAG-Based Codebase Indexing",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/196"
  },
  {
    "body": "## Goal\nImplement an agentic loop where MultiplAI can iterate multiple times on planning, coding, and testing until success.\n\n## Why This Matters\n- Current: Plan once → Code once → Fix if tests fail (max 3x)\n- Agentic: Plan → Code → Test → Reflect → Replan → Recode → ...\n- Enables handling ambiguous requirements and complex debugging\n\n## Implementation\n\n### Agentic Loop Architecture\n```\n                    ┌──────────────────────────────────────┐\n                    │                                      │\n                    ▼                                      │\n              ┌─────────┐                                  │\n              │  PLAN   │                                  │\n              └────┬────┘                                  │\n                   │                                       │\n                   ▼                                       │\n              ┌─────────┐                                  │\n              │  CODE   │                                  │\n              └────┬────┘                                  │\n                   │                                       │\n                   ▼                                       │\n              ┌─────────┐     fail                         │\n              │  TEST   │──────────┐                       │\n              └────┬────┘          │                       │\n                   │ pass          ▼                       │\n                   │         ┌──────────┐                  │\n                   │         │ REFLECT  │──────────────────┘\n                   │         └──────────┘   (replan needed)\n                   │               │\n                   │               │ (minor fix)\n                   │               ▼\n                   │         ┌─────────┐\n                   │         │   FIX   │───┐\n                   │         └─────────┘   │\n                   │                       │\n                   ▼                       ▼\n              ┌─────────┐           ┌─────────┐\n              │ REVIEW  │           │  TEST   │\n              └────┬────┘           └─────────┘\n                   │\n                   ▼\n                SUCCESS\n```\n\n### ReflectionAgent\n```typescript\n// src/agents/reflection.ts\ninterface ReflectionOutput {\n  diagnosis: string;           // What went wrong\n  rootCause: 'plan' | 'code' | 'test' | 'environment';\n  recommendation: 'replan' | 'fix' | 'abort';\n  feedback: string;            // Specific guidance for next iteration\n  confidence: number;          // 0-1, abort if too low after N attempts\n}\n\nclass ReflectionAgent extends BaseAgent<ReflectionInput, ReflectionOutput> {\n  async run(input: {\n    originalIssue: string;\n    plan: string[];\n    diff: string;\n    testOutput: string;\n    attemptNumber: number;\n  }): Promise<ReflectionOutput> {\n    // Analyze what went wrong\n    // Determine if it's a planning issue or implementation bug\n    // Provide actionable feedback\n  }\n}\n```\n\n### Loop Controller\n```typescript\n// src/core/agentic-loop.ts\ninterface LoopConfig {\n  maxIterations: 5;\n  maxReplans: 2;\n  confidenceThreshold: 0.3;  // Abort if confidence drops below\n}\n\nasync function agenticLoop(task: Task, config: LoopConfig): Promise<Result> {\n  let iteration = 0;\n  let replans = 0;\n  \n  while (iteration < config.maxIterations) {\n    const plan = await planner.run(task, previousFeedback);\n    const diff = await coder.run(plan);\n    const testResult = await foreman.run(diff);\n    \n    if (testResult.passed) {\n      return { success: true, diff };\n    }\n    \n    const reflection = await reflector.run({\n      plan, diff, testOutput: testResult.output\n    });\n    \n    if (reflection.confidence < config.confidenceThreshold) {\n      return { success: false, reason: 'Low confidence' };\n    }\n    \n    if (reflection.recommendation === 'replan') {\n      if (replans >= config.maxReplans) {\n        return { success: false, reason: 'Max replans exceeded' };\n      }\n      replans++;\n      previousFeedback = reflection.feedback;\n    } else {\n      // Fix attempt\n      diff = await fixer.run(diff, testResult.output, reflection.feedback);\n    }\n    \n    iteration++;\n  }\n}\n```\n\n### Memory Across Iterations\n- Track what was tried and failed\n- Prevent repeating same mistakes\n- Build up context about the problem\n\n## Definition of Done\n- [ ] ReflectionAgent implemented\n- [ ] Agentic loop controller\n- [ ] Iteration memory (what was tried)\n- [ ] Configurable limits and thresholds\n- [ ] Metrics: iterations per success, replan rate\n- [ ] Works on 3 complex test cases\n\n## Complexity: L\n## Estimate: 3-4 days",
    "number": 193,
    "title": "[Core] Agentic Loop with Self-Correction",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/193"
  },
  {
    "body": "## Overview\n\nExpose AutoDev functionality through a Model Control Protocol (MCP) server, enabling integration with AI-powered editors like Cursor, VS Code with Continue, and Windsurf.\n\n**Wave:** wave-3 (lowest priority, depends on core system working)  \n**Dependencies:** #136-#140 (Domain Memory), #131-#133 (Orchestration)  \n**Priority:** Low (nice-to-have, not critical path)\n\n## Key Insight from Domain Memory Pattern\n\n> \"Fewer, more orthogonal tools → more complex workflows become possible.\"\n\nThe MCP server should expose a **minimal, orthogonal tool set**:\n1. `autodev.analyze` - Run Initializer on an issue\n2. `autodev.execute` - Execute a task (full pipeline)\n3. `autodev.status` - Check task status\n4. `autodev.memory` - Query domain memory\n\n**NOT** 20 overlapping tools that confuse the host agent.\n\n## MCP Protocol Basics\n\n```typescript\n// MCP servers expose tools that AI agents can call\n// The protocol is JSON-RPC 2.0 over stdio\n\ninterface MCPTool {\n  name: string;\n  description: string;\n  inputSchema: JSONSchema;\n}\n\ninterface MCPToolCall {\n  name: string;\n  arguments: Record<string, unknown>;\n}\n\ninterface MCPToolResult {\n  content: ContentBlock[];\n  isError?: boolean;\n}\n```\n\n## AutoDev MCP Tools\n\n### Tool 1: autodev.analyze\n\nAnalyze a GitHub issue without executing.\n\n```typescript\nconst analyzeIssueTool: MCPTool = {\n  name: \"autodev.analyze\",\n  description: \"Analyze a GitHub issue and return the plan without executing. Use this to preview what AutoDev would do.\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      repo: { type: \"string\", description: \"owner/repo format\" },\n      issueNumber: { type: \"number\", description: \"GitHub issue number\" }\n    },\n    required: [\"repo\", \"issueNumber\"]\n  }\n};\n\n// Returns:\ninterface AnalyzeResult {\n  issue: { title: string; body: string };\n  analysis: {\n    complexity: \"XS\" | \"S\" | \"M\" | \"L\" | \"XL\";\n    targetFiles: string[];\n    acceptanceCriteria: string[];\n    plan: PlanStep[];\n    risks: RiskFactor[];\n    confidence: number;\n  };\n  recommendation: \"execute\" | \"breakdown\" | \"manual\";\n}\n```\n\n### Tool 2: autodev.execute\n\nExecute AutoDev on an issue (creates task, runs pipeline).\n\n```typescript\nconst executeIssueTool: MCPTool = {\n  name: \"autodev.execute\",\n  description: \"Execute AutoDev on a GitHub issue. Creates a task and runs the full pipeline (plan → code → test → review → PR).\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      repo: { type: \"string\" },\n      issueNumber: { type: \"number\" },\n      dryRun: { type: \"boolean\", description: \"If true, generate diff but don't create PR\" }\n    },\n    required: [\"repo\", \"issueNumber\"]\n  }\n};\n\n// Returns:\ninterface ExecuteResult {\n  taskId: string;\n  status: \"started\" | \"completed\" | \"failed\";\n  prUrl?: string;\n  diff?: string;  // If dryRun\n  error?: string;\n}\n```\n\n### Tool 3: autodev.status\n\nCheck status of a running or completed task.\n\n```typescript\nconst statusTool: MCPTool = {\n  name: \"autodev.status\",\n  description: \"Check the status of an AutoDev task.\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      taskId: { type: \"string\" }\n    },\n    required: [\"taskId\"]\n  }\n};\n\n// Returns:\ninterface StatusResult {\n  taskId: string;\n  status: TaskStatus;\n  phase: TaskPhase;\n  attempts: number;\n  progress: ProgressEntry[];\n  prUrl?: string;\n  lastError?: string;\n}\n```\n\n### Tool 4: autodev.memory\n\nQuery domain memory for context.\n\n```typescript\nconst memoryTool: MCPTool = {\n  name: \"autodev.memory\",\n  description: \"Query AutoDev's domain memory. Use to check repo configuration, past decisions, or learned patterns.\",\n  inputSchema: {\n    type: \"object\",\n    properties: {\n      repo: { type: \"string\" },\n      query: {\n        type: \"string\",\n        enum: [\"config\", \"recent_tasks\", \"patterns\", \"decisions\"]\n      }\n    },\n    required: [\"repo\", \"query\"]\n  }\n};\n\n// Returns based on query:\ntype MemoryResult = \n  | { type: \"config\"; data: StaticMemory }\n  | { type: \"recent_tasks\"; data: TaskSummary[] }\n  | { type: \"patterns\"; data: LearnedPattern[] }\n  | { type: \"decisions\"; data: Decision[] };\n```\n\n## MCP Server Implementation\n\n```typescript\n// src/mcp/server.ts\n\nimport { Server } from \"@modelcontextprotocol/sdk/server\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio\";\n\nconst server = new Server({\n  name: \"autodev\",\n  version: \"1.0.0\"\n}, {\n  capabilities: {\n    tools: {}\n  }\n});\n\n// Register tools\nserver.setRequestHandler(\"tools/list\", async () => ({\n  tools: [\n    analyzeIssueTool,\n    executeIssueTool,\n    statusTool,\n    memoryTool\n  ]\n}));\n\nserver.setRequestHandler(\"tools/call\", async (request) => {\n  const { name, arguments: args } = request.params;\n  \n  switch (name) {\n    case \"autodev.analyze\":\n      return await handleAnalyze(args);\n    case \"autodev.execute\":\n      return await handleExecute(args);\n    case \"autodev.status\":\n      return await handleStatus(args);\n    case \"autodev.memory\":\n      return await handleMemory(args);\n    default:\n      throw new Error(`Unknown tool: ${name}`);\n  }\n});\n\n// Start server\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n### Tool Handlers\n\n```typescript\n// src/mcp/handlers.ts\n\nasync function handleAnalyze(args: { repo: string; issueNumber: number }) {\n  const [owner, repo] = args.repo.split(\"/\");\n  const github = new GitHubClient(owner, repo);\n  \n  // Fetch issue\n  const issue = await github.getIssue(args.issueNumber);\n  \n  // Load static memory\n  const staticMemory = await staticStore.load({ owner, repo });\n  \n  // Run Initializer\n  const initializer = new InitializerAgent();\n  const analysis = await initializer.run({ issue, staticMemory });\n  \n  // Determine recommendation\n  const recommendation = \n    analysis.confidence.overall < 50 ? \"manual\" :\n    analysis.plan.complexity === \"M\" || analysis.plan.complexity === \"L\" ? \"breakdown\" :\n    \"execute\";\n  \n  return {\n    content: [{\n      type: \"text\",\n      text: JSON.stringify({\n        issue: { title: issue.title, body: issue.body },\n        analysis: {\n          complexity: analysis.plan.complexity,\n          targetFiles: analysis.fileAnalysis.targetFiles.map(f => f.path),\n          acceptanceCriteria: analysis.understanding.acceptanceCriteria.map(c => c.description),\n          plan: analysis.plan.steps,\n          risks: analysis.risks.factors,\n          confidence: analysis.confidence.overall\n        },\n        recommendation\n      }, null, 2)\n    }]\n  };\n}\n\nasync function handleExecute(args: { repo: string; issueNumber: number; dryRun?: boolean }) {\n  // Create task and trigger processing\n  const task = await createTaskFromIssue(args.repo, args.issueNumber);\n  \n  if (args.dryRun) {\n    // Run up to CODING_DONE but don't create PR\n    await orchestrator.processUntil(task.id, \"CODING_DONE\");\n    const session = await sessionStore.load(task.id);\n    \n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          taskId: task.id,\n          status: \"completed\",\n          diff: session.context.currentDiff\n        }, null, 2)\n      }]\n    };\n  }\n  \n  // Full execution (async - returns immediately)\n  orchestrator.process(task.id).catch(console.error);\n  \n  return {\n    content: [{\n      type: \"text\",\n      text: JSON.stringify({\n        taskId: task.id,\n        status: \"started\",\n        message: \"Task started. Use autodev.status to check progress.\"\n      }, null, 2)\n    }]\n  };\n}\n```\n\n## Editor Integration\n\n### Cursor Configuration\n\n```json\n// ~/.cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"autodev\": {\n      \"command\": \"bun\",\n      \"args\": [\"run\", \"/path/to/autodev/src/mcp/server.ts\"],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"${env:GITHUB_TOKEN}\",\n        \"ANTHROPIC_API_KEY\": \"${env:ANTHROPIC_API_KEY}\",\n        \"DATABASE_URL\": \"${env:DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n### VS Code with Continue\n\n```json\n// .continue/config.json\n{\n  \"models\": [...],\n  \"mcpServers\": [\n    {\n      \"name\": \"autodev\",\n      \"command\": \"bun run /path/to/autodev/src/mcp/server.ts\"\n    }\n  ]\n}\n```\n\n## Usage Examples\n\nIn Cursor/Continue chat:\n\n```\nUser: \"Analyze issue #42 in my-org/my-repo\"\n\nAgent: [calls autodev.analyze with repo=\"my-org/my-repo\", issueNumber=42]\n\nAutoDev returns:\n{\n  \"issue\": { \"title\": \"Add dark mode toggle\" },\n  \"analysis\": {\n    \"complexity\": \"S\",\n    \"targetFiles\": [\"src/components/ThemeToggle.tsx\", \"src/context/theme.ts\"],\n    \"confidence\": 85\n  },\n  \"recommendation\": \"execute\"\n}\n\nUser: \"Looks good, execute it\"\n\nAgent: [calls autodev.execute with repo=\"my-org/my-repo\", issueNumber=42]\n```\n\n## Implementation Steps\n\n1. Add `@modelcontextprotocol/sdk` dependency\n2. Create `src/mcp/server.ts` with basic MCP server\n3. Implement tool handlers for each tool\n4. Add stdio transport for CLI usage\n5. Create editor configuration examples\n6. Add README documentation for MCP setup\n7. Test with Cursor/Continue\n8. Handle authentication edge cases\n\n## Acceptance Criteria\n\n- [ ] MCP server starts and responds to tool/list\n- [ ] autodev.analyze returns issue analysis\n- [ ] autodev.execute creates task and starts processing\n- [ ] autodev.status returns task progress\n- [ ] autodev.memory queries domain memory\n- [ ] Works with Cursor\n- [ ] Works with Continue (VS Code)\n- [ ] Error handling for invalid inputs\n- [ ] Documentation for setup\n\n## Why Low Priority\n\nThis is a \"nice-to-have\" because:\n1. Core AutoDev works via webhooks (the main use case)\n2. MCP adds complexity without changing core functionality\n3. Editor integration requires user setup\n4. The API endpoints (`/api/tasks`, etc.) already provide similar functionality\n\nHowever, MCP enables:\n- Interactive exploration before execution\n- Better UX for developers who prefer editor integration\n- \"Dry run\" mode for previewing changes\n\n## Estimated Complexity\n\n**M** - MCP SDK integration + 4 tool handlers + editor configs.\n",
    "number": 135,
    "title": "[Integration] MCP Server - Editor Integration via Model Control Protocol",
    "url": "https://github.com/limaronaldo/MultiplAI/issues/135"
  }
]
